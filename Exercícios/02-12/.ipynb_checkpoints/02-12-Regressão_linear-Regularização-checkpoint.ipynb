{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão linear: Regularização\n",
    "\n",
    "## O que vamos fazer?\n",
    "- Implementar a função de custo regularizada para a regressão linear multivariável \n",
    "- Implementar a regularização para o gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação de um dataset sintético\n",
    "\n",
    "Para comprovar a sua implementação de uma função de custo e gradient descent regularizado, resgatar as suas células dos notebooks anteriores para os dataset sintéticos e gerar um dataset para este exercício.\n",
    "\n",
    "Não esquecer de acrescentar um termo de bias a *X* e um termo de erro a *Y*, inicializado a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta a estimar e as suas dimensões\n",
      "[0.85821972 0.07355824 0.62169114 0.2187661 ] (4,)\n",
      "\n",
      "Primeiras 10 filas e 5 colunas de X e Y:\n",
      "X: [[ 1.         73.18466991 22.38385299 42.79131065]\n",
      " [ 1.         52.85483023 65.72485892 69.57290066]\n",
      " [ 1.         49.82657813 42.28908332 24.3558428 ]\n",
      " [ 1.         31.69165285 39.21012308 99.13883681]\n",
      " [ 1.         48.66642219 11.90438595 36.27566465]\n",
      " [ 1.         92.91569268 77.11973636 88.83824868]\n",
      " [ 1.         73.14812027 55.66788444 98.13014093]\n",
      " [ 1.         36.48437995 97.26701745 82.3360809 ]\n",
      " [ 1.         57.2478898  29.51001085 43.43685771]\n",
      " [ 1.          4.85996975 61.24549027  9.61020007]]\n",
      "Y: [29.51868636 60.82688244 36.14235606 49.25420483 19.77477301 75.07238831\n",
      " 62.3146454  82.02433251 32.91789788 41.39387492]\n",
      "\n",
      "Dimensões de X e Y:\n",
      "(1000, 4) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Gerar um dataset sintéticos manualmente, com o termo de bias e o termo de erro inicializados a 0.\n",
    "\n",
    "m = 1000\n",
    "n = 3\n",
    "\n",
    "X = np.random.rand(m, n) * 100    # matriz com valores aleatórios de 0 a 100\n",
    "X = np.insert(X, 0, 1.0, axis=1)  # Adiciona a coluna de 1s\n",
    "\n",
    "Theta_verd = np.random.rand(n+1)\n",
    "\n",
    "Y = X @ Theta_verd\n",
    "\n",
    "# Comprovar os valores e dimensões dos vetores\n",
    "print('Theta a estimar e as suas dimensões') \n",
    "print(Theta_verd, Theta_verd.shape)\n",
    "print()\n",
    "\n",
    "print('Primeiras 10 filas e 5 colunas de X e Y:')\n",
    "print(\"X:\", X[:10])\n",
    "print(\"Y:\", Y[:10])\n",
    "print()\n",
    "\n",
    "print('Dimensões de X e Y:') \n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de custo regularizada\n",
    "\n",
    "Agora vamos modificar a nossa implementação da função de custo para adicionar o termo de regularização. \n",
    "\n",
    "Recordar que a função de custo regularizada é:\n",
    "\n",
    "$J_\\theta = \\frac{1}{2m} [\\sum\\limits_{i=0}^{m} (h_\\theta(x^i)-y^i)^2 + \\lambda \\sum\\limits_{j=1}^{n} \\theta^2_j]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementar a função de custo regularizada utilizando o seguinte modelo\n",
    "\n",
    "def regularized_cost_function(x, y, theta, lambda_=0.):\n",
    "    \"\"\" Computar a função de custo para o dataset e coeficientes considerados.\n",
    "    \n",
    "    Argumentos posicionais:\n",
    "     x -- array 2D de Numpy com os valores das variáveis independentes dos exemplos, de tamanho m x n \n",
    "    y -- array 1D Numpy com a variável dependente/objetivo, de tamanho m x 1\n",
    "    theta -- array 1D Numpy com os pesos dos coeficientes do modelo, de tamanho 1 x n (vetor fila)\n",
    "    \n",
    "    Argumentos numerados:\n",
    "    lambda -- float com o parâmetro de regularização\n",
    "    \n",
    "    Devolver:\n",
    "     j -- float com o custo regularizado para esse array theta \n",
    "     \"\"\"\n",
    "    theta = np.array(theta)\n",
    "    \n",
    "    m = len(x)\n",
    "    \n",
    "    # Recordar de verificar as dimensões da multiplicação da matriz para fazer corretamente.\n",
    "    # Recordar de não regularizar o coeficiente do parâmetro bias (primeiro valor do theta).\n",
    "    j = (np.sum((x @ theta.T - y) ** 2) + lambda_ * np.sum(theta**2)) / (2 * m)\n",
    "    \n",
    "    return j "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o dataset sintético tem o termo de erro a 0, a função de custo para o Theta_verd com o parâmetro lambda 0 deve ser exatamente 0.\n",
    "\n",
    "Como antes, à medida que nos afastamos com valores diferentes de theta, o custo deve aumentar. Do mesmo modo, quanto maior for o parâmetro de regularização, maior será a penalização e o custo, e quanto maior for o valor do theta, maior será a penalização e o custo.\n",
    "\n",
    "Comprovar a sua implementação nestas 5 circunstâncias:\n",
    "- Usando Theta_verd e com lambda a 0, o custo deve continuar a ser 0.\n",
    "- Com lambda 0 ainda, à medida que os valores theta se afastam de Theta_verd, o custo deve ser maior. \n",
    "- Usando Theta_verd e com lambda diferente de 0, o custo deve agora ser superior a 0.\n",
    "- Com lambda diferente de 0, para um theta diferente de Theta_verd o custo deve ser maior do que com lambda igual a 0.\n",
    "- Com lambda diferente de 0, quanto mais elevados forem os valores dos coeficientes de theta (positivos ou negativos*), maior será a penalização e maior será o custo.\n",
    "\n",
    "Recordar que o valor do lambda deve ser sempre positivo e inferior a 0: [0, 1e-1, 3e-1, 1e-2, 3e-2, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usando Theta_verd e com lambda a 0, o custo deve continuar a ser 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custo do modelo: 0.0\n",
      "Novo Theta [0.85821972 0.07355824 0.62169114 0.2187661 ]\n",
      "Theta Inicial [0.85821972 0.07355824 0.62169114 0.2187661 ]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Comprovar a implementação da sua função de custos regularizada nessas circunstâncias\n",
    "\n",
    "theta = Theta_verd \n",
    "j = regularized_cost_function(X, Y, theta) \n",
    "\n",
    "print('Custo do modelo:', j)\n",
    "print(\"Novo Theta\", theta)\n",
    "print(\"Theta Inicial\", Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Com lambda 0 ainda, à medida que os valores theta se afastam de Theta_verd, o custo deve ser maior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta: [2.85821972 2.07355824 2.62169114 2.2187661 ] \n",
      "Custo do modelo: 48392.13249548377\n"
     ]
    }
   ],
   "source": [
    "theta = Theta_verd + 2 # Modificar e testar vários valores do theta\n",
    "j = regularized_cost_function(X, Y, theta) \n",
    "print('Theta:', theta, '\\nCusto do modelo:', j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usando Theta_verd e com lambda diferente de 0, o custo deve agora ser superior a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.1 \n",
      "Custo do modelo: 5.881551900551085e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0.1\n",
    "j = regularized_cost_function(X, Y, Theta_verd, lambda_)\n",
    "print(\"Lambda:\", lambda_, \"\\nCusto do modelo:\", j)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Com lambda diferente de 0, para um theta diferente de Theta_verd o custo deve ser maior do que com lambda igual a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.1\n",
      "Theta: [2.85821972 2.07355824 2.62169114 2.2187661 ] \n",
      "Custo do modelo: 48392.13370874633\n"
     ]
    }
   ],
   "source": [
    "theta = Theta_verd + 2 # Modificar e testar vários valores do theta\n",
    "lambda_ = 0.1\n",
    "j = regularized_cost_function(X, Y, theta, lambda_) \n",
    "print(\"Lambda:\", lambda_)\n",
    "print('Theta:', theta, '\\nCusto do modelo:', j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Com lambda diferente de 0, quanto mais elevados forem os valores dos coeficientes de theta (positivos ou negativos*), maior será a penalização e maior será o custo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta: [1.85821972 1.07355824 1.62169114 1.2187661 ]\n",
      "Lambda: 0.1\n",
      "Custo do modelo: 12098.03355990998\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0.1\n",
    "theta = Theta_verd + 1 # Modificar e testar vários valores do theta\n",
    "j = regularized_cost_function(X, Y, theta, lambda_)\n",
    "print(\"Theta:\", theta)\n",
    "print(\"Lambda:\", lambda_)\n",
    "print(\"Custo do modelo:\", j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    theta = Theta_verd\n",
    "    alpha = 0 \n",
    "    Custo do modelo: 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado 2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    lambda = 0\n",
    "    Theta: [2.85821972 2.07355824 2.62169114 2.2187661 ] \n",
    "    Custo do modelo: 48392.13249548377\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    theta = Theta_verd\n",
    "    Lambda: 0.1 \n",
    "    Custo do modelo: 5.881551900551085e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "Lambda: 0.1    \r\n",
    "Theta: [2.85821972 2.07355824 2.62169114 2.2187661 ]     \r\n",
    "Custo do modelo: 48392.13370874633"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Theta: [1.85821972 1.07355824 1.62169114 1.2187661 ]\n",
    "    Lambda: 0.1\n",
    "    Custo do modelo: 12098.03355990998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent regularizado\n",
    "\n",
    "Agora vamos regularizar também a formação por gradient descent. Vamos modificar as atualizações de *Theta* para que agora contenham também o parâmetro de regularização *lambda*:\n",
    "\n",
    "$\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_0^i \\\\\n",
    "\\theta_j := \\theta_j - \\alpha [\\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_j^i + \\frac{\\lambda}{m} \\theta_j] \\\\\n",
    "\\theta_j := \\theta_j (1 - \\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_j^i \\\\\n",
    "j \\in [1, n]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementar a função que forma o modelo por gradient descent regularizado\n",
    "\n",
    "def regularized_gradient_descent(x, y, theta, alpha, lambda_=0., e, iter_):\n",
    "    \"\"\" Formar o modelo otimizando a sua função de custo por gradient descent\n",
    "    \n",
    "    Argumentos posicionais:\n",
    "    x -- array 2D de Numpy com os valores das variáveis independentes dos exemplos, de tamanho m x n \n",
    "    y -- array 1D Numpy com a variável dependente/objetivo, de tamanho m x 1\n",
    "    theta -- array 1D Numpy com os pesos dos coeficientes do modelo, de tamanho 1 x n (vetor fila) \n",
    "    alpha -- float, ratio de formação\n",
    "    \n",
    "    Argumentos numerados (keyword):\n",
    "    lambda -- float com o parâmetro de regularização\n",
    "    e -- float, diferença mínima entre iterações para declarar que a formação finalmente convergiu \n",
    "    iter_ -- int/float, número de iterações\n",
    "    \n",
    "    Devolver:\n",
    "    j_hist -- list/array com a evolução da função de custo durante a formação \n",
    "    theta -- array Numpy com o valor do theta na última iteração\n",
    "    \"\"\"\n",
    "    # TODO: declarar valores por defeito para e e iter_ nos argumentos nomeados (palavra-chave) da função.\n",
    "    \n",
    "    iter_ = int(iter_) # Se declarou iter_ em notação científica (1e3) ou float (1000.), converter\n",
    "    \n",
    "    # Inicializar j_hist como uma list ou um array Numpy. Recordar que não sabemos que tamanho terá eventualmente\n",
    "    j_hist = [...]\n",
    "    \n",
    "    m, n = [...] # Obter m e n a partir das dimensões de X\n",
    "   \n",
    "    for k in [...]: # Iterar sobre o número máximo de iterações\n",
    "        theta_iter = [...] # Declarar um theta para cada iteração, pois precisamos de a atualizar.\n",
    "        \n",
    "        for j in [...]: # Iterar sobre o número de características\n",
    "            # Atualizar theta_iter para cada característica, de acordo com a derivada da função de custo\n",
    "            # Incluir o ratio de formação alpha\n",
    "            # Cuidado com as multiplicações matriciais, a sua ordem e dimensões\n",
    "            \n",
    "            if j > 0:\n",
    "                pass # Regularizar tudo coeficiente exceto o do parâmetro bias (primeiro coef.)\n",
    "            \n",
    "            theta_iter[j] = theta[j] - [...] \n",
    "            \n",
    "        theta = theta_iter\n",
    "        \n",
    "        cost = cost_function([...]) # Calcular o custo para a atual iteração theta\n",
    "        \n",
    "        j_hist[...] # Adicionar o custo da iteração atual ao histórico de custos.\n",
    "        \n",
    "        # Comprovar se a diferença entre o custo da iteração atual e o custo da última iteração em valor absoluto é  inferior à diferença mínima para declarar a convergência, e\n",
    "        # absoluto são inferiores que a diferença mínima para declarar a convergência, e\n",
    "        if k > 0 and [...]:\n",
    "            print('Convergir na iteração n.º: ', k)\n",
    "            break\n",
    "    else:\n",
    "        print('N.º máx. de iterações alcançado')\n",
    "        \n",
    "    return j_hist, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_gradient_descent(x, y, theta, alpha, lambda_=0., e=1e-3, iter_=1000):\n",
    "    \"\"\" Formar o modelo otimizando a sua função de custo por gradient descent \"\"\"\n",
    "    iter_ = int(iter_)  # Converter para inteiro\n",
    "    j_hist = []  # Histórico de custos\n",
    "    \n",
    "    m, n = x.shape  # Obter m e n a partir das dimensões de X\n",
    "    \n",
    "    for k in range(iter_):\n",
    "        theta_iter = theta.copy()  # Copiar theta para a iteração atual\n",
    "        \n",
    "        # Calcular a hipótese\n",
    "        h = x @ theta_iter\n",
    "        \n",
    "        for j in range(n):  # Iterar sobre o número de características\n",
    "            if j == 0:\n",
    "                # Atualizar theta_0 (não regularizado)\n",
    "                theta_iter[j] = theta[j] - (alpha / m) * np.sum(h - y)\n",
    "            else:\n",
    "                # Atualizar theta_j (regularizado)\n",
    "                theta_iter[j] = theta[j] - (alpha / m) * (np.sum(h - y) * x[:, j] + (lambda_ * theta[j]))\n",
    "        \n",
    "        theta = theta_iter  # Atualizar theta para a próxima iteração\n",
    "        \n",
    "        # Calcular o custo para a iteração atual\n",
    "        cost = regularized_cost_function(x, y, theta, lambda_)\n",
    "        j_hist.append(cost)  # Adicionar o custo ao histórico\n",
    "        \n",
    "        # Verificar a convergência\n",
    "        if k > 0 and abs(j_hist[-1] - j_hist[-2]) < e:\n",
    "            print('Convergir na iteração n.º:', k)\n",
    "            break\n",
    "    else:\n",
    "        print('N.º máx. de iterações alcançado')\n",
    "        \n",
    "    return j_hist, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste Versao2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_gradient_descent(x, y, theta, alpha, lambda_=0., e=1e-3, iter_=1000):\n",
    "    \"\"\" Formar o modelo otimizando a sua função de custo por gradient descent \"\"\"\n",
    "    iter_ = int(iter_)  # Converter para inteiro\n",
    "    j_hist = []  # Histórico de custos\n",
    "    \n",
    "    m, n = x.shape  # Obter m e n a partir das dimensões de X\n",
    "    \n",
    "    for k in range(iter_):\n",
    "        theta_iter = theta.copy()  # Copiar theta para a iteração atual\n",
    "        \n",
    "        # Calcular a hipótese\n",
    "        h = x @ theta_iter\n",
    "        \n",
    "        # Atualizar theta_0 (não regularizado)\n",
    "        theta_iter[0] = theta[0] - (alpha / m) * np.sum(h - y)\n",
    "        \n",
    "        # Atualizar theta_j (regularizado) para j = 1, ..., n\n",
    "        for j in range(1, n):\n",
    "            theta_iter[j] = theta[j] * (1 - (alpha * lambda_ / m)) - (alpha / m) * np.sum((h - y) * x[:, j])\n",
    "        \n",
    "        theta = theta_iter  # Atualizar theta para a próxima iteração\n",
    "        \n",
    "        # Calcular o custo para a iteração atual\n",
    "        cost = regularized_cost_function(x, y, theta, lambda_)\n",
    "        j_hist.append(cost)  # Adicionar o custo ao histórico\n",
    "        \n",
    "        # Verificar a convergência\n",
    "        if k > 0 and abs(j_hist[-1] - j_hist[-2]) < e:\n",
    "            print('Convergir na iteração n.º:', k)\n",
    "            break\n",
    "    else:\n",
    "        print('N.º máx. de iterações alcançado')\n",
    "        \n",
    "    return j_hist, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versao 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_gradient_descent(x, y, theta, alpha, lambda_=0., e=1e-3, iter_=1e3):\n",
    "    \"\"\" Formar o modelo otimizando a sua função de custo por gradient descent regularizado \"\"\"\n",
    "    \n",
    "    iter_ = int(iter_)  # converter para inteiro se necessário\n",
    "    m, n = x.shape\n",
    "    j_hist = []\n",
    "\n",
    "    for k in range(iter_):\n",
    "        theta_iter = theta.copy()\n",
    "        h = x @ theta  # predição h(x)\n",
    "        error = h - y  # erro\n",
    "\n",
    "        for j in range(n):\n",
    "            grad = (1 / m) * np.sum(error * x[:, j])\n",
    "            if j == 0:\n",
    "                theta_iter[j] = theta[j] - alpha * grad  # sem regularização para θ0\n",
    "            else:\n",
    "                reg_term = (lambda_ / m) * theta[j]\n",
    "                theta_iter[j] = theta[j] * (1 - alpha * lambda_ / m) - alpha * grad\n",
    "\n",
    "        theta = theta_iter\n",
    "        cost = regularized_cost_function(x, y, theta, lambda_)\n",
    "        j_hist.append(cost)\n",
    "\n",
    "        if k > 0 and abs(j_hist[-2] - j_hist[-1]) < e:\n",
    "            print('Convergência alcançada na iteração:', k)\n",
    "            break\n",
    "    else:\n",
    "        print('N.º máx. de iterações alcançado')\n",
    "\n",
    "    return j_hist, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nota*: Recordar que os modelos de código são apenas uma ajuda. Ocasionalmente, poderá querer usar códigos diferentes com a mesma funcionalidade, por exemplo, iterar sobre elementos de uma forma diferente, etc. Sinta-se à vontade para os modificar como desejar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprovar a sua implementação, mais uma vez, comprovar com *lambda* usando vários valores de *Theta*, tanto o *Theta_verd* como valores cada vez mais afastados do mesmo, e comprovar se eventualmente o modelo converge para o *Theta_verd* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta inicial:\n",
      "[ 0.66200211 -0.25358023 -1.06055696 -0.25274972]\n",
      "Hiper-parâmetros usados:\n",
      "Alpha: 0.1 | Erro máx.: 0.001 | Nº iterações: 1000.0\n",
      "\n",
      "Media: 1.0 \n",
      "Desv. Padrao: 0.0 \n",
      "Estabilidade: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan]\n",
      "\n",
      "N.º máx. de iterações alcançado\n",
      "Tempo de formação (s): 0.06934142112731934\n",
      "\n",
      "Últimos 10 valores da função de custo:\n",
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "\n",
      "Custo final:\n",
      "nan\n",
      "\n",
      "Theta final:\n",
      "[nan nan nan nan]\n",
      "\n",
      "Valores verdadeiros de Theta e diferença:\n",
      "[0.85821972 0.07355824 0.62169114 0.2187661 ]\n",
      "[nan nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SBUtilizador\\AppData\\Local\\Temp\\ipykernel_9760\\3879356454.py:18: RuntimeWarning: invalid value encountered in divide\n",
      "  x = (x_ - x_mean) / x_std\n",
      "C:\\Users\\SBUtilizador\\AppData\\Local\\Temp\\ipykernel_9760\\4112150294.py:23: RuntimeWarning: overflow encountered in square\n",
      "  j = (np.sum((x @ theta.T - y) ** 2) + lambda_ * np.sum(theta**2)) / (2 * m)\n",
      "C:\\Users\\SBUtilizador\\AppData\\Local\\Temp\\ipykernel_9760\\2287352215.py:25: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  if k > 0 and abs(j_hist[-2] - j_hist[-1]) < e:\n",
      "C:\\Users\\SBUtilizador\\AppData\\Local\\Temp\\ipykernel_9760\\2287352215.py:19: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  theta_iter[j] = theta[j] * (1 - alpha * lambda_ / m) - alpha * grad\n"
     ]
    }
   ],
   "source": [
    "theta_ini = np.random.randn(n + 1)\n",
    "\n",
    "print('Theta inicial:')\n",
    "print(theta_ini)\n",
    "\n",
    "alpha = 1e-1\n",
    "lambda_ = 0.1\n",
    "e = 1e-3\n",
    "iter_ = 1e3\n",
    "\n",
    "print('Hiper-parâmetros usados:')\n",
    "print('Alpha:', alpha, '| Erro máx.:', e, '| Nº iterações:', iter_)\n",
    "\n",
    "print()\n",
    "x_ = X[:, 0]\n",
    "x_mean = np.mean(x_, axis=0)\n",
    "x_std = np.std(x_, axis=0)\n",
    "x = (x_ - x_mean) / x_std\n",
    "print(\"Media:\", x_mean, \"\\nDesv. Padrao:\", x_std, \"\\nEstabilidade:\", x)\n",
    "\n",
    "print()\n",
    "\n",
    "# Treinar modelo\n",
    "t = time.time()\n",
    "j_hist, theta_final = regularized_gradient_descent(X, Y, theta_ini, alpha, lambda_, e, iter_)\n",
    "print('Tempo de formação (s):', time.time() - t)\n",
    "\n",
    "# Resultados\n",
    "print('\\nÚltimos 10 valores da função de custo:')\n",
    "print(j_hist[-10:])\n",
    "\n",
    "print('\\nCusto final:')\n",
    "print(j_hist[-1])\n",
    "\n",
    "print('\\nTheta final:')\n",
    "print(theta_final)\n",
    "\n",
    "print('\\nValores verdadeiros de Theta e diferença:')\n",
    "print(Theta_verd)\n",
    "print(theta_final - Theta_verd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Testar a sua implementação através da formação de um modelo no dataset sintético anteriormente criado.\n",
    "\n",
    "# Criar um theta inicial com um determinado valor.\n",
    "theta_ini = [...]\n",
    "\n",
    "print('Theta inicial:') \n",
    "print(theta_ini)\n",
    "\n",
    "alpha = 1e-1 \n",
    "lambda_ = 0. \n",
    "e = 1e-3\n",
    "iter_ = 1e3 # Verificar se a sua função pode suportar valores de float ou modificá-los.\n",
    "\n",
    "print('Hiper-parâmetros usados:')\n",
    "print('Alpha:', alpha, 'Error máx.:', e, 'Nº iter', iter_)\n",
    "\n",
    "t = time.time()\n",
    "j_hist, theta_final = regularized_gradient_descent([...]) \n",
    "\n",
    "print('Tempo de formação (s):', time.time() - t)\n",
    "\n",
    "# TODO: completar\n",
    "print('\\nÚltimos 10 valores da função de custo') \n",
    "print(j_hist[...])\n",
    "print('\\Custo final:') \n",
    "print(j_hist[...]) \n",
    "print('\\nTheta final:') \n",
    "print(theta_final)\n",
    "\n",
    "print('Valores verdadeiros de Theta e diferença com valores formados:') \n",
    "print(Theta_verd)\n",
    "print(theta_final - Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora confirmar novamente a formação de um modelo em algumas das circunstâncias acima referidas:\n",
    "\n",
    "- Usando um theta_ini aleatório e com lambda a 0, o custo final deverá ser ainda próximo de 0 e o theta final próximo de Theta_verd. \n",
    "- Usando um theta_ini aleatório e com lambda pequeno e diferente de 0, o custo final deve ser próximo de 0, embora o modelo possa começar a perder precisão.\n",
    "- À medida que o valor lambda aumenta, o modelo vai perdendo mais precisão.\n",
    "\n",
    "Lembre-se que pode alterar os valores das células e voltar a executar as células. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anotar os seus resultados nesta célula:\n",
    "\n",
    "1. Resultado1\n",
    "1. Resultado2\n",
    "1. Resultado3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m54",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m54"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
