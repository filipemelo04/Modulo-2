{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão linear: Validação cruzada e avaliação final\n",
    "\n",
    "Finalmente, iremos formar um modelo de regressão linear completa, aplicando o pré-processamento de dados, verificando o modelo por validação cruzada, avaliando-o num subset de testes e, finalmente, fazendo previsões com ele.\n",
    "\n",
    "Este é, portanto, um exemplo completo de como formar um modelo de regressão linear multivariável.\n",
    "\n",
    "## O que vamos fazer?\n",
    "- Criar um dataset sintético para regressão linear multivariável \n",
    "- Reprocessar os dados\n",
    "- Formar o modelo no subset de formação e verificar a sua adequação \n",
    "- Encontrar o hiper-parâmetro lambda ótimo no subset de validação cruzada ou CV \n",
    "- Avaliar o modelo no subset de teste\n",
    "- Fazer previsões sobre novos exemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar um dataset sintético para regressão linear\n",
    "\n",
    "Vamos começar, como habitualmente, por criar um dataset sintético para este exercício\n",
    "\n",
    "Criar um manualmente com um termo de erro modificável, não esquecendo o seu termo de bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensao Theta: (4,)\n",
      "Theta: [3.18837502 1.91516479 6.65796855 4.84542686]\n",
      "\n",
      "Primeiras 10 filas e 5 colunas de X e Y:\n",
      "X: [[ 1.         -0.56695651 -0.28414695 -0.55657645]\n",
      " [ 1.         -0.99336079  0.88374542  0.63568676]\n",
      " [ 1.         -0.93501466  0.66492104 -0.89912375]\n",
      " [ 1.          0.82906932 -0.43341289 -0.05990409]\n",
      " [ 1.          0.48452726  0.76571436 -0.00277069]\n",
      " [ 1.         -0.16466784  0.27243874  0.76432147]\n",
      " [ 1.          0.0968995  -0.06579606 -0.27267523]\n",
      " [ 1.          0.05814736  0.25641192  0.59848575]\n",
      " [ 1.          0.27410141  0.59919333 -0.80181373]\n",
      " [ 1.         -0.10216436  0.17313255  0.76635028]]\n",
      "Y: [-2.59940773 11.47446668  1.65346645  1.49374094  9.06329873  9.71500363\n",
      "  1.29237615  7.20226328  4.25416477  6.58998752]\n",
      "\n",
      "Dimensao de X: (1000, 4)\n",
      "Dimensao de Y: (1000,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Gerar um dataset sintéticos manualmente, com o termo de bias e o termo de erro\n",
    "\n",
    "m = 1000\n",
    "n = 3\n",
    "\n",
    "# Criar uma matriz de números aleatórios no intervalo [-1, 1)\n",
    "X = np.random.uniform(-1, 1, (m, n))\n",
    "# Inserir um vetor de 1s como primeira coluna de X\n",
    "X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "Theta_verd = np.random.rand(n + 1) * 10 \n",
    "\n",
    "# Computar Y multiplicando os vetores X e Theta \n",
    "Y = np.matmul(X, Theta_verd)\n",
    "\n",
    "error = 0.2\n",
    "\n",
    "# Calcular Y com erro\n",
    "termino_error = np.random.uniform(-1, 1, size=len(Y)) * error\n",
    "Y = Y + Y * termino_error\n",
    "\n",
    "# Comprovar os valores e dimensões dos vetores\n",
    "print(\"Dimensao Theta:\", Theta_verd.shape)\n",
    "print(\"Theta:\", Theta_verd)\n",
    "\n",
    "print()\n",
    "\n",
    "print('Primeiras 10 filas e 5 colunas de X e Y:')\n",
    "print(\"X:\", X[:10])\n",
    "print(\"Y:\", Y[:10]) \n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Dimensao de X:\", X.shape)\n",
    "print(\"Dimensao de Y:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processar os dados\n",
    "\n",
    "Vamos pré-processar os dados por completo, para os preparar.\n",
    "\n",
    "Desta vez, iremos seguir os passos seguintes para pré-processar os dados:\n",
    "- Reordená-los aleatoriamente. \n",
    "- Normalizá-los.\n",
    "- Dividi-los em subsets de formação, validação cruzada e de teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reordenar o dataset aleatoriamente\n",
    "\n",
    "Desta vez, vamos utilizar um dataset sintético criado com base em dados aleatórios. Por conseguinte, não seria necessário reordená-los, uma vez que, sendo aleatórios, já se encontram desorganizados por defeito.\n",
    "\n",
    "No entanto, podemos normalmente deparar-nos com dataset reais cujos dados têm uma certa ordem, um padrão, o que pode confundir a nossa formação.\n",
    "\n",
    "Portanto, sempre antes de começarmos a tratar os dados, a primeira coisa que fazemos é reordená-los aleatoriamente, principalmente antes de os dividirmos nos subsets de formação, CV e teste.\n",
    "\n",
    "*Nota*: Muito importante, recordar sempre de reordenar sempre *X* e *Y* na mesma ordem, para que a cada exemplo seja atribuído o mesmo resultado antes e depois da reordenação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeiras 10 filas e 5 colunas de X e Y:\n",
      "X: [[ 1.         -0.56695651 -0.28414695 -0.55657645]\n",
      " [ 1.         -0.99336079  0.88374542  0.63568676]\n",
      " [ 1.         -0.93501466  0.66492104 -0.89912375]\n",
      " [ 1.          0.82906932 -0.43341289 -0.05990409]\n",
      " [ 1.          0.48452726  0.76571436 -0.00277069]\n",
      " [ 1.         -0.16466784  0.27243874  0.76432147]\n",
      " [ 1.          0.0968995  -0.06579606 -0.27267523]\n",
      " [ 1.          0.05814736  0.25641192  0.59848575]\n",
      " [ 1.          0.27410141  0.59919333 -0.80181373]\n",
      " [ 1.         -0.10216436  0.17313255  0.76635028]]\n",
      "Y: [-2.59940773 11.47446668  1.65346645  1.49374094  9.06329873  9.71500363\n",
      "  1.29237615  7.20226328  4.25416477  6.58998752]\n",
      "Reordenamos X e Y:\n",
      "Primeiras 10 filas e 5 colunas de X e Y:\n",
      "X: [[ 1.          0.88466959  0.01199735 -0.17875024]\n",
      " [ 1.         -0.45627649 -0.99240467  0.36805799]\n",
      " [ 1.          0.49738    -0.8029932   0.46849825]\n",
      " [ 1.          0.19790748 -0.40942067  0.68975091]\n",
      " [ 1.          0.8346368  -0.51616987  0.98354635]\n",
      " [ 1.         -0.77297822 -0.89677785 -0.95162077]\n",
      " [ 1.          0.90416283 -0.59176036  0.90778978]\n",
      " [ 1.          0.14466746 -0.00350796 -0.92004032]\n",
      " [ 1.         -0.41982253 -0.38698059 -0.96787854]\n",
      " [ 1.         -0.62565108 -0.20045624  0.95557664]]\n",
      "Y: [ 3.73253456 -2.78734027  0.91675273  4.96875236  7.10401188 -8.31322365\n",
      "  5.7591019  -0.98652696 -5.55634589  5.38119459]\n",
      "Dimensões de X e Y:\n",
      "(1000, 4) (1000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# TODO: Reordenar aleatoriamente o dataset\n",
    "\n",
    "print('Primeiras 10 filas e 5 colunas de X e Y:') \n",
    "print(\"X:\", X[:10])\n",
    "print(\"Y:\", Y[:10])\n",
    "\n",
    "print('Reordenamos X e Y:')\n",
    "# Se preferir, pode usar a função sklearn.utils.shuffle convenience function.\n",
    "# Usar um estado inicial aleatório de 42, de modo a manter a reprodutibilidade.\n",
    "X, Y = shuffle(X, Y, random_state=42)\n",
    "\n",
    "print('Primeiras 10 filas e 5 colunas de X e Y:') \n",
    "print(\"X:\", X[:10])\n",
    "print(\"Y:\", Y[:10])\n",
    "\n",
    "print('Dimensões de X e Y:') \n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprovar se *X* e *Y* têm as dimensões corretas e uma ordem diferente da anterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizar o dataset\n",
    "\n",
    "Uma vez reordenados os dados aleatoriamente, vamos proceder à normalização do dataset de \n",
    "exemplos *X*.\n",
    "\n",
    "Para o fazer, copiar as células de código dos exercícios anteriores para o normalizar.\n",
    "\n",
    "*Nota*: Em exercícios anteriores utilizávamos 2 células de código diferentes, uma para definir a função de normalização e outra para normalizar o dataset. Pode combinar ambas as células numa única célula para guardar este pré-processamento numa célula reutilizável no futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.020155794424534745, 0.29072126249360203, 0.36955942946206044]\n",
      "X original:\n",
      "[[ 1.          0.88466959  0.01199735 -0.17875024]\n",
      " [ 1.         -0.45627649 -0.99240467  0.36805799]\n",
      " [ 1.          0.49738    -0.8029932   0.46849825]\n",
      " ...\n",
      " [ 1.          0.68863513 -0.40342078  0.28564326]\n",
      " [ 1.         -0.9095763   0.5203206  -0.40124442]\n",
      " [ 1.         -0.0041393   0.88953358  0.65216492]]\n",
      "(1000, 4)\n",
      "Média e desvio típico das características:\n",
      "[-0.020155794424534745, 0.29072126249360203, 0.36955942946206044]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMédia e desvio típico das características:\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(mu)\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28mprint\u001b[39m(mu\u001b[38;5;241m.\u001b[39mshape) \n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(std) \n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(std\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# TODO: Normalizar o dataset ocom uma sua função de normalização.\n",
    "\n",
    "def normalize(x, mu, std):\n",
    "    \"\"\" Normalizar um dataset com exemplos X\n",
    "    \n",
    "    Argumentos posicionais:\n",
    "    x -- array 2D de Numpy com os exemplos, sem termo de bias\n",
    "    mu -- vetor 1D de Numpy com a média de cada característica/coluna\n",
    "    std -- vetor 1D de Numpy com o desvio típico de cada característica/coluna\n",
    "    \n",
    "    Devolver:\n",
    "    X norm -- array 2D de Numpy com os exemplos, com as suas características normalizadas \n",
    "    \"\"\"\n",
    "    return [...]\n",
    "\n",
    "# Encontrar a média e o desvio padrão das características de X (colunas), exceto a primeira (parcialidade).\n",
    "mu = []\n",
    "for i in range(len(X[0])-1):\n",
    "    mu.append(np.mean(X[i+1]))\n",
    "print(mu)\n",
    "std = [...]\n",
    "\n",
    "print('X original:') \n",
    "print(X) \n",
    "print(X.shape)\n",
    "\n",
    "print('Média e desvio típico das características:') \n",
    "print(mu)\n",
    "print(mu.shape) \n",
    "print(std) \n",
    "print(std.shape)\n",
    "\n",
    "print('X normalizada:') \n",
    "X_norm = np.copy(X)\n",
    "X_norm[...] = normalize(X[...], mu, std) # Normalizar apenas a coluna 1 e as colunas seguintes, não a coluna 0.\n",
    "print(X_norm) \n",
    "print(X_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nota*: Algumas pessoas preferem calcular a média *mu* e o desvio padrão *std* de cada característica/coluna de *X* na mesma função de normalização se não forem incluídos como argumentos (o que seria então opcional), devolvendo os 3 valores, uma vez que para fazer previsões precisamos de normalizar os dados com a mesma *mu* e *std* originais.\n",
    "\n",
    "Se preferir, pode modificar a sua implementação da função para o fazer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir os dataset e subset de formação, CV e testes\n",
    "\n",
    "Finalmente, vamos dividir o dataset nos 3 subset a serem utilizados.\n",
    "\n",
    "Para isso, vamos utilizar uma proporção de 60%/20%/20%, uma vez que estamos a partir de 1 000 exemplos. Como dissemos, para um número diferente de exemplos, podemos modificar a proporção:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Dividir o dataset X e Y nos 3 subconjuntos, de acordo com os ratios indicados.\n",
    "\n",
    "ratios = [60,20,20]\n",
    "print('Ratios:\\n', ratios, ratios[0] + ratios[1] + ratios[2])\n",
    "\n",
    "r = [0,0]\n",
    "# Dica: a função round() e o atributo x.shape podem ser úteis para si\n",
    "r[0] = [...]\n",
    "r[1] = [...]\n",
    "print('Índices de corte:\\n', r)\n",
    "\n",
    "# Dica: a função np.array_split() pode ser útil para si\n",
    "X_train, X_cv, X_test = [...] \n",
    "Y_train, Y_cv, Y_test = [...]\n",
    "\n",
    "print('Tamanhos dos subsets:') \n",
    "print(X_train.shape) \n",
    "print(Y_train.shape) \n",
    "print(X_cv.shape) print(Y_cv.shape) \n",
    "print(X_test.shape) \n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formar um modelo inicial sobre o subset de formação.\n",
    "\n",
    "Antes de começarmos a otimizar o hiper-parâmetro *lambda*, iremos formar um modelo inicial não regularizado no subset de formação, para verificar o seu desempenho e adequação, e para ter a certeza de que faz sentido formar um modelo ML de regressão linear multivariável em tal dataset, uma vez que as características podem não ser adequadas, pode haver uma relação baixa entre elas, podem não seguir uma relação linear, etc.\n",
    "\n",
    "\n",
    "Para o fazer, vamos seguir os passos seguintes:\n",
    "- Formar um modelo inicial, sem regularização, com *lambda* a 0.\n",
    "- Representar o histórico do seu custo para comprovar a sua evolução. \n",
    "- Voltar a formar o modelo se necessário, por exemplo, variando o ratio de aprendizagem *alpha*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copia las celdas de ejercicios anteriores donde implementabas las funciones de coste y gradient descent regularizadas, y copia la celda donde entrenabas el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copiar as células com custo regularizado e funções de gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copiar a célula onde formamos o modelo\n",
    "# Formar o seu modelo no subconjunto de formação não regularizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da mesma forma que anteriormente, verificar a formação do modelo, representando graficamente a evolução da função de custo de acordo com o número de iterações, copiando a célula de código correspondente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representar a evolução da função de custo vs o número de iterações\n",
    "\n",
    "plt.figure(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como dissemos anteriormente, reveja a formação do seu modelo e modifique alguns parâmetros, se necessário, para o voltar a formar, procurando um bom desempenho: o ratio de aprendizagem, o ponto de convergência, o número máximo de iterações, etc., com exceção do parâmetro de regularização *lambda*, que deve ser fixado em 0.\n",
    "\n",
    "*Nota*: Este ponto é importante, visto que estes hiper-parâmetros serão geralmente os mesmos que utilizaremos para o resto da \n",
    "otimização do modelo, pelo que agora é o momento de encontrar os valores certos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprovar se existe desvio ou sobreajuste, *bias* ou *variância*\n",
    "\n",
    "Há um teste que podemos fazer rapidamente para verificar se o nosso modelo inicial sofre claramente de parcialidade, variação, ou se tem um desempenho mais ou menos aceitável.\n",
    "\n",
    "Vamos representar graficamente a evolução da função de custo de 2 modelos, um formado sobre os primeiros n exemplos do subset de formação e o outro formado sobre os primeiros n exemplos do subset de validação cruzada.\n",
    "\n",
    "Uma vez que o subset de formação e o subset de validação cruzada não têm o mesmo tamanho, utiliza apenas o mesmo número de exemplos para este subset do que o número total de exemplos no subset CV.\n",
    "\n",
    "Para o fazer, formar 2 modelos em condições de igualdade, copiando novamente as células de código correspondentes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO: Estabelecer um theta_ini e híper-parâmetros comuns para ambos os modelos, a fim de os formar em igualdade de\n",
    "# condições\n",
    "\n",
    "theta_ini = [...]\n",
    "\n",
    "print('Theta inicial:') \n",
    "print(theta_ini)\n",
    "\n",
    "alpha = 1e-1 \n",
    "lambda_ = 0. \n",
    "e = 1e-3 \n",
    "iter_ = 1e3\n",
    "\n",
    "print('Hiper-parâmetros usados:')\n",
    "print('Alpha:', alpha, 'Error máx.:', e, 'Nº iter', iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO: Formar um modelo sem regularização nos primeiros n valores do X_train, onde n é o número de\n",
    "# exemplos disponíveis em X_cv\n",
    "# Usar j_hist_train e theta_train como nomes de variáveis para os distinguir do outro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nota*: Comprovar se o theta_ini não foi modificado, ou modificar o seu código para que ambos os modelos utilizem o mesmo theta_ini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Da mesma forma, formar um modelo sem regularização em X_cv com os mesmos parâmetros.\n",
    "# Recordar de usar j_hist_cv e theta_cv como nomes de variável."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora representar ambas as evoluções no mesmo gráfico, com cores diferentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representar num gráfico de linhas ambas as evoluções para comparação.\n",
    "\n",
    "plt.figure(2)\n",
    "\n",
    "plt.title() \n",
    "plt.xlabel() \n",
    "plt.ylabel()\n",
    "\n",
    "# Usa colores diferentes para ambas series, e indica una legenda para os distinguir\n",
    "plt.plot() \n",
    "plt.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com um dataset sintético aleatórios é difícil que um ou outro seja o caso, mas nesta forma poderíamos apreciar tais problemas da seguinte forma:\n",
    "\n",
    "Se o custo final em ambos os subset for elevado, pode haver um problema de desvio ou *bias*.\n",
    "Se o custo final em ambos os subset for muito diferente um do outro, pode haver um problema de sobreajuste ou *variação*.\n",
    "\n",
    "Recordar o que ambos significavam:\n",
    "- O desvio ocorre quando o modelo não consegue ajustar suficientemente bem na curva do dataset, ou porque não são as características certas (ou porque faltam outras), ou porque os dados têm demasiados erros, ou porque o modelo segue uma relação diferente ou é demasiado simples.\n",
    "- O sobreajuste ocorre quando o modelo se ajusta demasiado bem à curva do dataset, demasiado bem, demasiado próximo dos exemplos sobre os quais foi formado, e quando tem de prever novos resultados, não o faz corretamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprovar a adequação do modelo\n",
    "\n",
    "Como dissemos, outro motivo para formar um modelo inicial é comprovar se faz sentido formar um modelo de regressão linear multivariável nesse dataset. Se virmos que o modelo sofre de sobreajuste, podemos sempre o corrigir com a regularização. No entanto, se virmos que sofre de um elevado \n",
    "\n",
    "Se virmos que o modelo sofre de sobreajuste, podemos sempre o corrigir com a regularização. No entanto, se virmos que sofre de um elevado desvio, ou seja, que o custo final é muito elevado, o nosso tipo de modelo ou as características escolhidas podem não ser adequados para este problema.\n",
    "\n",
    "Neste caso, descobrimos que o erro é suficientemente baixo para o tornar promissor para continuar a formar um modelo de regressão linear multivariável como este"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encontrar o hiper-parâmetro lambda ótimo no subset de validação cruzada\n",
    "\n",
    "Agora, a fim de encontrar o lambda ideal, iremos formar um modelo diferente para cada valor de lambda a ser considerado, no subset de formação, e verificar a sua exatidão no subset de validação cruzada. \n",
    "\n",
    "Vamos representar graficamente o erro ou custo final de cada modelo vs o valor de lambda usado, para ver qual modelo tem um erro ou custo inferior no subset de validação cruzada.\n",
    "\n",
    "Deste modo, formamos todos os modelos no mesmo subset e em condições iguais (exceto lambda), e avaliamo-los num subset de dados que não tenham visto anteriormente, que não utilizámos para os formar.\n",
    "\n",
    "O subset CV não é, portanto, utilizado para formar o modelo, mas apenas para avaliar o valor lambda ótimo. Exceto, como fizemos no ponto anterior, para fazer uma rápida avaliação inicial da possível ocorrência de sobreajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Formar um modelo para cada valor lambda diferente em X_train e avaliá-lo em X_cv\n",
    "\n",
    "lambdas = [0., 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1, 1e0, 3e0, 1e1]\n",
    "\n",
    "# Completar o código para formar um modelo diferente para cada valor de lambda no X_train\n",
    "# Armazenar o theta e erro/custo final\n",
    "# Posteriormente, avaliar o seu custo total no subset da CV\n",
    "\n",
    "# Armazenar essa informação nas seguintes matrizes, do mesmo tamanho que os lambdas\n",
    "j_train = [...]\n",
    "j_cv = [...]\n",
    "theta_cv = [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez todos os modelos formados, representar num gráfico de linhas o seu custo final sobre o subset de formação e o custo final sobre o subset de CV vs o valor *lambda* utilizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representar graficamente o erro final para cada valor de lambda\n",
    "\n",
    "plt.figure(3)\n",
    "\n",
    "# Completar com o seu código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez representados estes erros finais, podemos escolher manualmente o modelo com o valor *lambda* ótimo, ou podemos fazê-lo de forma automatizada com código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Escolher o modelo ótimo e o valor lambda, com o menor erro no subset do CV\n",
    "\n",
    "# Iterar sobre todas as combinações de theta e lambda e escolher o custo mais baixo no subset do CV\n",
    "\n",
    "j_final = [...] \n",
    "theta_final = [...] \n",
    "lambda_final = [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez implementadas todas as etapas acima mencionadas, temos o nosso modelo formado e os seus hiper -parâmetros otimizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliar o modelo finalmente sobre o subset de teste\n",
    "\n",
    "Finalmente, encontrámos os nossos coeficientes theta e hiper-parâmetro lambda ótimos, pelo que temos agora um modelo formado pronto a ser usado.\n",
    "\n",
    "No entanto, embora tenhamos calculado o seu erro ou custo final no subset CV, utilizámos este subset para escolher o modelo, para terminar a “Formação”, para atuar sobre o modelo. Portanto, ainda não testamos como este modelo irá funcionar com dados que nunca viu antes.\n",
    "\n",
    "Por isso, vamos finalmente avaliá-lo no subset de teste, num subset que ainda não utilizámos, nem para formar o modelo, nem para escolher os seus hiper-parâmetros. Um subset separado que a formação do modelo ainda não viu.\n",
    "\n",
    "Para tal, vamos calcular o erro ou custo total no subset de teste e verificar graficamente os resíduos do modelo no mesmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcular o erro do modelo no subset de teste usando a função de custo com a correspondente\n",
    "# theta e lambda\n",
    "\n",
    "j_test = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcular as previsões do modelo no subset de teste, calcular os resíduos e representá-los\n",
    "\n",
    "Y_test_pred = [...]\n",
    "\n",
    "residuos = [...]\n",
    "\n",
    "plt.figure(4)\n",
    "\n",
    "# Completar com o seu código\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desta forma, podemos ter uma ideia mais realista da precisão do nosso modelo e do seu comportamento com novos exemplos no futuro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazer previsões sobre novos exemplos\n",
    "\n",
    "Com o nosso modelo formado, otimizado e avaliado, tudo o que resta é pô-lo a funcionar, fazendo previsões com novos exemplos.\n",
    "\n",
    "Para isso, vamos:\n",
    "- Gerar um novo exemplo seguindo o mesmo padrão que o dataset original. \n",
    "- Normalizar as suas características antes de poder fazer previsões sobre eles. \n",
    "- Gerar uma previsão para esse novo exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO: Gerar um novo exemplo seguindo o padrão original, com termo de bias e erro aleatório.\n",
    "\n",
    "X_pred = [...]\n",
    "\n",
    "# Normalizar as suas características (exceto o termo de bias) com as médias e desvios típicos originais\n",
    "X_pred = [...]\n",
    "\n",
    "# Gerar uma previsão para esse exemplo.\n",
    "Y_pred = [...]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m54",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m54"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
