{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão logística: Função de custo e formação\n",
    "\n",
    "## O que vamos fazer?\n",
    "- Criar um dataset sintético para regressão logística manualmente e com Scikit-learn. \n",
    "- Implementar a função de ativação logística sigmoide.\n",
    "- Implementar a função de custo regularizado para a regressão logística. \n",
    "- Implementar a formação do modelo por gradient descent.\n",
    "- Comprovar a formação representando a evolução da função custo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import make_classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação de um dataset sintético para regressão logística\n",
    "\n",
    "Vamos criar novamente um dataset sintéticos, mas desta vez para regressão logística.\n",
    "\n",
    "Vamos descobrir como fazê-lo com os 2 métodos que utilizámos anteriormente: manualmente e com Scikit-learn, usando a função  [sklearn_datasets.make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta a estimar\n",
      "Dimensão: (3,)\n",
      "[1.51294845 4.99522456 2.06491143]\n",
      "\n",
      "Primeiras 5 linhas de X e Y:\n",
      "X:  [[ 1.          0.73308866 -0.30609581]\n",
      " [ 1.         -0.3402037   0.96215158]\n",
      " [ 1.         -0.76665703  0.5905477 ]\n",
      " [ 1.         -0.29391683  0.85534818]\n",
      " [ 1.         -0.29409367  0.34714683]]\n",
      "\n",
      "Y:  [1. 1. 0. 1. 1.]\n",
      "\n",
      "Dimensões de X e Y:\n",
      "X:  (100, 3)\n",
      "Y:  (100,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Gerar um dataset sintético, com o termo de bias e erro de forma manual\n",
    "m = 100\n",
    "n = 2\n",
    "\n",
    "# Gerar um array 2D m x n com valores números aleatórios entre -1 e 1.\n",
    "# Inserir o termo de bias como primeira coluna de 1s\n",
    "\n",
    "X = np.random.rand(m, n) * 2 - 1\n",
    "X = np.insert(X, 0, 1.0, axis=1)  \n",
    "\n",
    "# Gerar um array de theta de n + 1 valores aleatórios\n",
    "Theta_verd = np.random.rand(n+1) * 10\n",
    "\n",
    "# Calcular Y em função de X e Theta_verd\n",
    "# Adicionar um termo de erro modificável\n",
    "# Transformar Y para valores de 1. e 0. (float) quando Y >= 0,0\n",
    "error = 0.15\n",
    "termino_error = np.random.uniform(-1, 1, size=len(X)) * error\n",
    "\n",
    "Y = np.matmul(X, Theta_verd)\n",
    "Y = Y + termino_error\n",
    "Y = (Y >= 0).astype(float)  # classe 1 se Y >= 0, senão 0\n",
    "# Comprovar os valores e dimensões dos vetores\n",
    "print('Theta a estimar') \n",
    "print(\"Dimensão:\", Theta_verd.shape)\n",
    "print(Theta_verd)\n",
    "\n",
    "print()\n",
    "\n",
    "print('Primeiras 5 linhas de X e Y:') \n",
    "print(\"X: \", X[:5])\n",
    "print()\n",
    "print(\"Y: \", Y[:5])\n",
    "\n",
    "print()\n",
    "\n",
    "print('Dimensões de X e Y:') \n",
    "print(\"X: \", X.shape)\n",
    "print(\"Y: \", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeiras 5 linhas de X_sklearn e Y_sklearn:\n",
      "X_sklearn:  [[ 1.         -0.81632788 -0.50396685]\n",
      " [ 1.         -0.70770938 -0.31709018]\n",
      " [ 1.          0.82661658  0.18912822]\n",
      " [ 1.         -0.12025905  0.60882099]\n",
      " [ 1.         -0.79767625 -0.64670389]]\n",
      "\n",
      "Y_sklearn:  [0. 0. 1. 1. 0.]\n",
      "\n",
      "Dimensões de X_sklearn e Y_sklearn:\n",
      "X_sklearn:  (100, 3)\n",
      "Y_sklearn:  (100,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Gerar um dataset sintéticos, com o termo de bias e erro com Scikit-learn\n",
    "\n",
    "# Utilizar os mesmos valores de m, n e erro do dataset anterior\n",
    "X_sklearn, Y_sklearn = make_classification(\n",
    "    n_samples=m,\n",
    "    n_features=n,\n",
    "    n_informative=n,    # todas as features são informativas\n",
    "    n_redundant=0,       # sem features redundantes\n",
    "    n_classes=2,\n",
    "    flip_y=error,        \n",
    "    class_sep=1.0,       # separação entre classes\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Comprovar os valores e dimensões dos vetores\n",
    "print('Primeiras 5 linhas de X_sklearn e Y_sklearn:') \n",
    "print(\"X_sklearn: \", X[:5])\n",
    "print()\n",
    "print(\"Y_sklearn: \", Y[:5])\n",
    "\n",
    "print()\n",
    "\n",
    "print('Dimensões de X_sklearn e Y_sklearn:') \n",
    "print(\"X_sklearn: \", X.shape)\n",
    "print(\"Y_sklearn: \", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que com o método Scikit-learn não podemos recuperar os coeficientes utilizados, vamos usar o método manual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementar a função sigmoide\n",
    "\n",
    "Vamos implementar a função de ativação sigmoide. Vamos usar esta função para implementar a nossa hipótese, que transforma as previsões do modelo em valores de 0 e 1.\n",
    "\n",
    "Função sigmoide:\n",
    "\n",
    "$g(z) = \\frac{1}{1 + e^{-z}} \\\\\n",
    "Y = h_\\theta(x) = g(\\Theta \\times X) = \\frac{1}{1 + e^{-\\Theta^Tx}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementar a função de ativação sigmóide.\n",
    "\n",
    "def sigmoid(theta, x):\n",
    "    \"\"\" Devolver o valor do sigmoide para essa theta y x\n",
    "        Argumentos posicionais:\n",
    "        theta -- array 1D de Numpy com a fila ou coluna de coeficientes das características \n",
    "        x -- array 1D de Numpy com as características de um exemplo\n",
    "        Devolver:\n",
    "        sigmoide -- float com o valor do sigmoide para esses parâmetros\n",
    "    \"\"\"\n",
    "\n",
    "    z = x @ theta  # Calcular o produto escalar entre theta e x\n",
    "    y = 1 / (1 + np.exp(-z))  # Aplicar a função sigmoide\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementar a função de custo regularizada\n",
    "\n",
    "Vamos implementar a função de custo regularizada. Esta função será semelhante à que implementámos para a regressão linear num exercício anterior\n",
    "\n",
    "Função de custo regularizada\n",
    "\n",
    "$J(\\Theta) = - [\\frac{1}{m} \\sum\\limits_{i=0}^{m} (y^i log(h_\\theta(x^i)) + (1 - y^i) log(1 - h_\\theta(x^i))] \\\\\n",
    "+ \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\Theta_j^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementar a função de custo regularizado para a regressão logística\n",
    "\n",
    "def regularized_logistic_cost_function(x, y, theta, lambda_=0.):\n",
    "    \"\"\" Computar a função de custo para o dataset e coeficientes considerados\n",
    "    \n",
    "    Argumentos posicionais:\n",
    "    i -- array 2D de Numpy com os valores das variáveis independentes dos exemplos, de tamanho m x n \n",
    "    y -- array 1D Numpy com a variável dependente/objetivo, 1 e valores 0 ou 1\n",
    "    theta -- array 1D Numpy com os pesos dos coeficientes do modelo, de tamanho 1 x n (vetor fila) \n",
    "    lambda_ -- fator de regularização, por defeito 0.\n",
    "    \n",
    "    Devolver:\n",
    "    j -- float com o custo para esse array theta \n",
    "    \"\"\"\n",
    "    m = len(y)  # Número de exemplos\n",
    "    epsilon = 1e-8 # Adiciona um pequeno valor epsilon para proteger o log\n",
    "    \n",
    "    # Calcular g(z) usando a função sigmoide\n",
    "    g = sigmoid(theta, x)  # Transpor x para multiplicação correta\n",
    "    \n",
    "    # Calcular a função de custo sem regularização\n",
    "    j = -((1/(m)) * (np.sum(y * np.log(g + epsilon))) + (1 - y) @ np.log(1 - g + epsilon))\n",
    "    \n",
    "    # Adicionar o termo de regularização (excluindo o primeiro coeficiente)\n",
    "    j += (lambda_ / (2 * m)) * np.sum(theta ** 2) #np.sum(theta[1:] ** 2)\n",
    "    \n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como em exercícios anteriores, comprovar a sua implementação calculando a função de custo para cada exemplo do dataset.\n",
    "\n",
    "Com o Y correto e a *lambda* a 0, a função de custo também deve ser 0. À medida que o *theta* se afasta ou a *lambda* aumenta, o custo deve ser superior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custo do modelo com Theta_verd:\n",
      "6.47364211707124\n",
      "\n",
      "Custo do modelo com theta_test:\n",
      "2.9809327727825865\n",
      "\n",
      "Theta comprovado e Theta real:\n",
      "[ 8.06328481 24.13429334  8.33181343]\n",
      "[1.51294845 4.99522456 2.06491143]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Comprovar a sua implementação no dataset\n",
    "\n",
    "# Modificar e comprovar vários valores de theta \n",
    "#theta = Theta_verd\n",
    "theta_test = np.array([8.06328481, 24.13429334,  8.33181343])\n",
    "j_verd = regularized_logistic_cost_function(X, Y, Theta_verd, lambda_=0.5) \n",
    "j_test = regularized_logistic_cost_function(X, Y, theta_test, lambda_=0.5) \n",
    "\n",
    "print('Custo do modelo com Theta_verd:')\n",
    "print(j_verd)\n",
    "print()\n",
    "print('Custo do modelo com theta_test:')\n",
    "print(j_test)\n",
    "print()\n",
    "print('Theta comprovado e Theta real:') \n",
    "print(theta)\n",
    "print(Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementar a formação por gradient descent\n",
    "\n",
    "Vamos agora otimizar esta função de custos, para formar o nosso modelo através de gradient descent \n",
    "regularizado. \n",
    "\n",
    "No exercício seguinte vamos usar a regularização para efetuar a validação cruzada.\n",
    "\n",
    "Atualizações dos coeficientes *theta*:\n",
    "\n",
    "$\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=0}^{m} (h_\\theta (x^i) - y^i) x_0^i \\\\\n",
    "\\theta_j := \\theta_j - \\alpha [\\frac{1}{m} \\sum\\limits_{i=0}^{m} (h_\\theta (x^i) - y^i) x_0^i + \\frac{\\lambda}{m} \\theta_j]; \\\\\n",
    "j \\in [1, n]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementar a função que forma o modelo por gradient descent regularizado\n",
    "\n",
    "def regularized_logistic_gradient_descent(x, y, theta, alpha=1e-1, lambda_=0., e=0.001, iter_=0.001): \n",
    "    \"\"\" \n",
    "    Formar o modelo otimizando a sua função de custo por gradient descent\n",
    "    \n",
    "    Argumentos posicionais:\n",
    "    x -- array 2D de Numpy com os valores das variáveis independentes dos exemplos, de tamanho m x n \n",
    "    y -- array 1D Numpy com a variável dependente/objetivo, de tamanho m x 1\n",
    "    theta -- array 1D Numpy com os pesos dos coeficientes do modelo, de tamanho 1 x n (vetor fila)\n",
    "    \n",
    "    Argumentos numerados (keyword):\n",
    "    alpha -- float, ratio de formação\n",
    "    lambda -- float com o parâmetro de regularização\n",
    "    e -- float, diferença mínima entre iterações para declarar que a formação finalmente convergiu \n",
    "    iter_ -- int/float, número de iterações\n",
    "    \n",
    "    Devolver:\n",
    "    j_hist -- list/array com a evolução da função de custo durante a formação \n",
    "    theta -- array Numpy com o valor do theta na última iteração\n",
    "    \"\"\"\n",
    "    iter = int(iter_)  # Converter iter_ para inteiro se necessário\n",
    "    j_hist = []  # Inicializar histórico de custos\n",
    "    \n",
    "    m, n = x.shape  # Obter m e n a partir das dimensões de X\n",
    "    \n",
    "    for k in range(iter):  # Iterar sobre o número máximo de iterações\n",
    "        h = sigmoid(theta, x)  # Calcular a hipótese h_theta(x)\n",
    "        error = h - y  # Calcular o erro\n",
    "        \n",
    "        # Atualizar theta\n",
    "        theta_iter = np.copy(theta)  # Copiar theta para atualização\n",
    "        for j in range(n):  # Iterar sobre o número de características\n",
    "            if j == 0:\n",
    "                # Não regularizar o termo de bias\n",
    "                theta_iter[j] = theta[j] - (alpha / m) * np.dot(error, x[:, j])\n",
    "            else:\n",
    "                # Regularizar todos os outros coeficientes\n",
    "                theta_iter[j] = theta[j] - (alpha / m) * (np.dot(error, x[:, j]) + (lambda_ * theta[j]))\n",
    "        \n",
    "        theta = theta_iter  # Atualizar theta para a próxima iteração\n",
    "        \n",
    "        # Calcular o custo para a iteração atual\n",
    "        cost = regularized_logistic_cost_function(x, y, theta, lambda_)\n",
    "        j_hist.append(cost)  # Adicionar o custo ao histórico\n",
    "        \n",
    "        # Verificar a convergência\n",
    "        if k > 0 and abs(j_hist[-1] - j_hist[-2]) < e:\n",
    "            print('Convergir na iteração n.º: ', k)\n",
    "            break\n",
    "    else:\n",
    "        print('N.º máx. de iterações alcançado')\n",
    "    \n",
    "    return j_hist, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formar um modelo de regressão logística não regularizado\n",
    "\n",
    "Para comprovar a implementação da sua função, utilize-o para formar um modelo de regressão logística no dataset sintéticos sem regularização (*lambda* = 0).\n",
    "\n",
    "Comprovar se o modelo converge corretamente para *Theta_verd*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta inicial:\n",
      "[16. 20.  9.]\n",
      "Hiper-parâmetros usados:\n",
      "Alpha: 0.5 Error máx.: 0.0001 Nº iter 10000.0\n",
      "Convergir na iteração n.º:  1055\n",
      "Tempo de formação (s): 0.08667397499084473\n",
      "\n",
      "Últimos 10 valores da função de custo\n",
      "[1.193173295441158, 1.193072008883093, 1.1929708920633089, 1.1928699445662727, 1.19276916597843, 1.192668555888181, 1.1925681138858673, 1.1924678395637516, 1.192367732516007, 1.1922677923387033]\n",
      "\n",
      "Custo final:\n",
      "1.1922677923387033\n",
      "\n",
      "Theta final:\n",
      "[ 8.06328481 24.13429334  8.33181343]\n",
      "Valores verdadeiros de Theta e diferença com valores formados:\n",
      "[1.51294845 4.99522456 2.06491143]\n",
      "[ 6.55033636 19.13906878  6.266902  ]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Comprovar a sua implementação através da formação de um modelo no dataset sintético anteriormente criado.\n",
    "\n",
    "# Criar um theta inicial com um determinado valor.\n",
    "theta_ini = np.array([16.0, 20.0, 9.0])\n",
    "\n",
    "print('Theta inicial:') \n",
    "print(theta_ini)\n",
    "\n",
    "alpha = 0.5\n",
    "lambda_ = 0.001 \n",
    "e = 1e-4\n",
    "iter_ = 1e4\n",
    "\n",
    "print('Hiper-parâmetros usados:')\n",
    "print('Alpha:', alpha, 'Error máx.:', e, 'Nº iter', iter_)\n",
    "\n",
    "t = time.time()\n",
    "j_hist, theta_final = regularized_logistic_gradient_descent(X, Y, theta_ini, alpha, lambda_, e, iter_) \n",
    "\n",
    "print('Tempo de formação (s):', time.time() - t)\n",
    "\n",
    "# TODO: completar\n",
    "print('\\nÚltimos 10 valores da função de custo') \n",
    "print(j_hist[-10:])\n",
    "print('\\nCusto final:') \n",
    "print(j_hist[-1]) \n",
    "print('\\nTheta final:') \n",
    "print(theta_final)\n",
    "\n",
    "print('Valores verdadeiros de Theta e diferença com valores formados:') \n",
    "print(Theta_verd)\n",
    "print(theta_final - Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representar a evolução da função de custo \n",
    "\n",
    "Para comprovar a evolução da formação do seu modelo, representar graficamente o histórico da função custo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOOD: Representar graficamente a função de custo vs. o número de iterações\n",
    "plt.figure()\n",
    "\n",
    "plt.title('Função de custo') \n",
    "plt.xlabel('nº iterações') \n",
    "plt.ylabel('custo')\n",
    "\n",
    "plt.plot([...]) # Completar\n",
    "\n",
    "plt.grid() \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m54",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m54"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
