{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão logística: Função de custo e formação\n",
    "\n",
    "## O que vamos fazer?\n",
    "- Criar um dataset sintético para regressão logística manualmente e com Scikit-learn. \n",
    "- Implementar a função de ativação logística sigmoide.\n",
    "- Implementar a função de custo regularizado para a regressão logística. \n",
    "- Implementar a formação do modelo por gradient descent.\n",
    "- Comprovar a formação representando a evolução da função custo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import make_classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação de um dataset sintético para regressão logística\n",
    "\n",
    "Vamos criar novamente um dataset sintéticos, mas desta vez para regressão logística.\n",
    "\n",
    "Vamos descobrir como fazê-lo com os 2 métodos que utilizámos anteriormente: manualmente e com Scikit-learn, usando a função  [sklearn_datasets.make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta a estimar\n",
      "Dimensão: (3,)\n",
      "[7.33609677 2.25545573 1.12675069]\n",
      "\n",
      "Primeiras 5 linhas de X e Y:\n",
      "X:  [[ 1.          0.81642541 -0.59387298]\n",
      " [ 1.          0.35690632 -0.40007418]\n",
      " [ 1.          0.23337156 -0.60960638]\n",
      " [ 1.          0.96603028  0.34756497]\n",
      " [ 1.          0.19297659 -0.34331895]]\n",
      "\n",
      "Y:  [1. 1. 1. 1. 1.]\n",
      "\n",
      "Dimensões de X e Y:\n",
      "X:  (100, 3)\n",
      "Y:  (100,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Gerar um dataset sintético, com o termo de bias e erro de forma manual\n",
    "m = 100\n",
    "n = 2\n",
    "\n",
    "# Gerar um array 2D m x n com valores números aleatórios entre -1 e 1.\n",
    "# Inserir o termo de bias como primeira coluna de 1s\n",
    "\n",
    "X = np.random.rand(m, n) * 2 - 1\n",
    "X = np.insert(X, 0, 1.0, axis=1)  \n",
    "\n",
    "# Gerar um array de theta de n + 1 valores aleatórios\n",
    "Theta_verd = np.random.rand(n+1) * 10\n",
    "\n",
    "# Calcular Y em função de X e Theta_verd\n",
    "# Adicionar um termo de erro modificável\n",
    "# Transformar Y para valores de 1. e 0. (float) quando Y >= 0,0\n",
    "error = 0.15\n",
    "termino_error = np.random.uniform(-1, 1, size=len(X)) * error\n",
    "\n",
    "Y = np.matmul(X, Theta_verd)\n",
    "Y = Y + termino_error\n",
    "Y = (Y >= 0).astype(float)  # classe 1 se Y >= 0, senão 0\n",
    "# Comprovar os valores e dimensões dos vetores\n",
    "print('Theta a estimar') \n",
    "print(\"Dimensão:\", Theta_verd.shape)\n",
    "print(Theta_verd)\n",
    "\n",
    "print()\n",
    "\n",
    "print('Primeiras 5 linhas de X e Y:') \n",
    "print(\"X: \", X[:5])\n",
    "print()\n",
    "print(\"Y: \", Y[:5])\n",
    "\n",
    "print()\n",
    "\n",
    "print('Dimensões de X e Y:') \n",
    "print(\"X: \", X.shape)\n",
    "print(\"Y: \", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeiras 5 linhas de X_sklearn e Y_sklearn:\n",
      "X_sklearn:  [[ 1.          0.67006512  0.38236029]\n",
      " [ 1.          0.73473423 -0.98412619]\n",
      " [ 1.          0.00289049 -0.83589334]\n",
      " [ 1.         -0.91218636 -0.11885557]\n",
      " [ 1.          0.24695537 -0.75105906]]\n",
      "\n",
      "Y_sklearn:  [1. 1. 1. 1. 1.]\n",
      "\n",
      "Dimensões de X_sklearn e Y_sklearn:\n",
      "X_sklearn:  (100, 3)\n",
      "Y_sklearn:  (100,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Gerar um dataset sintéticos, com o termo de bias e erro com Scikit-learn\n",
    "\n",
    "# Utilizar os mesmos valores de m, n e erro do dataset anterior\n",
    "X_sklearn, Y_sklearn = make_classification(\n",
    "    n_samples=m,\n",
    "    n_features=n,\n",
    "    n_informative=n,    # todas as features são informativas\n",
    "    n_redundant=0,       # sem features redundantes\n",
    "    n_classes=2,\n",
    "    flip_y=error,        # introduz erro nas classes (ex: 15% de ruído)\n",
    "    class_sep=1.0,       # separação entre classes\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Comprovar os valores e dimensões dos vetores\n",
    "print('Primeiras 5 linhas de X_sklearn e Y_sklearn:') \n",
    "print(\"X_sklearn: \", X[:5])\n",
    "print()\n",
    "print(\"Y_sklearn: \", Y[:5])\n",
    "\n",
    "print()\n",
    "\n",
    "print('Dimensões de X_sklearn e Y_sklearn:') \n",
    "print(\"X_sklearn: \", X.shape)\n",
    "print(\"Y_sklearn: \", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que com o método Scikit-learn não podemos recuperar os coeficientes utilizados, vamos usar o método manual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementar a função sigmoide\n",
    "\n",
    "Vamos implementar a função de ativação sigmoide. Vamos usar esta função para implementar a nossa hipótese, que transforma as previsões do modelo em valores de 0 e 1.\n",
    "\n",
    "Função sigmoide:\n",
    "\n",
    "$g(z) = \\frac{1}{1 + e^{-z}} \\\\\n",
    "Y = h_\\theta(x) = g(\\Theta \\times X) = \\frac{1}{1 + e^{-\\Theta^Tx}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementar a função de ativação sigmóide.\n",
    "\n",
    "def sigmoid(theta, x):\n",
    "    \"\"\" Devolver o valor do sigmoide para essa theta y x\n",
    "        Argumentos posicionais:\n",
    "        theta -- array 1D de Numpy com a fila ou coluna de coeficientes das características \n",
    "        x -- array 1D de Numpy com as características de um exemplo\n",
    "        Devolver:\n",
    "        sigmoide -- float com o valor do sigmoide para esses parâmetros\n",
    "    \"\"\"\n",
    "\n",
    "    z = x @ theta  # Calcular o produto escalar entre theta e x\n",
    "    y = 1 / (1 + np.exp(-z))  # Aplicar a função sigmoide\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.99997798e-01, 1.65703326e-01, 9.99990138e-01, 1.00000000e+00,\n",
       "       2.21552317e-04, 9.42351656e-01, 2.68222350e-02, 9.69080692e-01,\n",
       "       9.58172822e-01, 9.99999920e-01, 7.11023710e-01, 1.00000000e+00,\n",
       "       9.99916832e-01, 9.99997795e-01, 9.99999725e-01, 9.99316918e-01,\n",
       "       9.99999541e-01, 1.00000000e+00, 9.97352976e-01, 9.99999994e-01,\n",
       "       1.15549342e-01, 5.97411388e-03, 1.06178393e-03, 2.76681222e-03,\n",
       "       9.97112925e-01, 9.99999931e-01, 9.99999881e-01, 9.99883385e-01,\n",
       "       2.12734142e-03, 9.99999687e-01, 3.66448721e-03, 1.00000000e+00,\n",
       "       9.69252042e-01, 9.99999999e-01, 9.99999876e-01, 9.96129103e-01,\n",
       "       9.99997532e-01, 9.95002478e-01, 9.93545638e-01, 9.97539482e-01,\n",
       "       9.99999938e-01, 1.00000000e+00, 9.99368750e-01, 9.99988183e-01,\n",
       "       9.99989318e-01, 9.99989286e-01, 9.99999997e-01, 9.99879091e-01,\n",
       "       9.99999941e-01, 9.99998472e-01, 9.99997493e-01, 9.99999994e-01,\n",
       "       9.99994728e-01, 9.99999696e-01, 9.99873015e-01, 4.93343339e-01,\n",
       "       6.96228586e-06, 1.00000000e+00, 1.00000000e+00, 5.31565976e-02,\n",
       "       1.00000000e+00, 1.43851192e-02, 2.52037521e-07, 9.99824630e-01,\n",
       "       8.36898207e-05, 1.00000000e+00, 9.99832438e-01, 3.96137011e-06,\n",
       "       2.58081725e-01, 9.79054237e-01, 1.00000000e+00, 9.99986195e-01,\n",
       "       9.99999996e-01, 5.30930627e-01, 9.99995223e-01, 1.77715112e-01,\n",
       "       1.00000000e+00, 9.99998448e-01, 9.99904594e-01, 9.99999874e-01,\n",
       "       9.99998485e-01, 8.90422208e-01, 9.99999998e-01, 9.99999603e-01,\n",
       "       2.16926048e-01, 9.99998053e-01, 9.99256655e-01, 1.00000000e+00,\n",
       "       9.98447611e-01, 9.99999596e-01, 9.99948506e-01, 4.22530988e-01,\n",
       "       9.99991051e-01, 9.99995711e-01, 1.00000000e+00, 9.99990170e-01,\n",
       "       9.92688955e-01, 2.45821864e-02, 9.99999826e-01, 1.67240822e-06])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid([9.37670897, 12.57506796, 16.25229096], X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementar a função de custo regularizada\n",
    "\n",
    "Vamos implementar a função de custo regularizada. Esta função será semelhante à que implementámos para a regressão linear num exercício anterior\n",
    "\n",
    "Função de custo regularizada\n",
    "\n",
    "$J(\\Theta) = - [\\frac{1}{m} \\sum\\limits_{i=0}^{m} (y^i log(h_\\theta(x^i)) + (1 - y^i) log(1 - h_\\theta(x^i))] \\\\\n",
    "+ \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\Theta_j^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementar a função de custo regularizado para a regressão logística\n",
    "\n",
    "def regularized_logistic_cost_function(x, y, theta, lambda_=0.):\n",
    "    \"\"\" Computar a função de custo para o dataset e coeficientes considerados\n",
    "    \n",
    "    Argumentos posicionais:\n",
    "    i -- array 2D de Numpy com os valores das variáveis independentes dos exemplos, de tamanho m x n \n",
    "    y -- array 1D Numpy com a variável dependente/objetivo, 1 e valores 0 ou 1\n",
    "    theta -- array 1D Numpy com os pesos dos coeficientes do modelo, de tamanho 1 x n (vetor fila) \n",
    "    lambda_ -- fator de regularização, por defeito 0.\n",
    "    \n",
    "    Devolver:\n",
    "    j -- float com o custo para esse array theta \n",
    "    \"\"\"\n",
    "    m = len(y)  # Número de exemplos\n",
    "    epsilon = 1e-8 # Adiciona um pequeno valor epsilon para proteger o log\n",
    "    \n",
    "    # Calcular a hipótese h_theta(x) usando a função sigmoide\n",
    "    h = sigmoid(theta, x)  # Transpor x para multiplicação correta\n",
    "    \n",
    "    # Calcular a função de custo sem regularização\n",
    "    j = (-1/m) * np.sum(y * np.log(h + epsilon) + (1 - y) * np.log(1 - h + epsilon))\n",
    "    \n",
    "    # Adicionar o termo de regularização (excluindo o primeiro coeficiente)\n",
    "    j += (lambda_ / (2 * m)) * np.sum(theta[1:] ** 2)\n",
    "    \n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def regularized_logistic_cost_function(x, y, theta, lambda_=0.):\n",
    "    m = x.shape[0]\n",
    "    print(m)\n",
    "    h = sigmoid(theta, x.T)\n",
    "    #h = 1 / (1 + np.exp(-x @ theta))\n",
    "    cost = (-1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "    reg = (lambda_/(2*m)) * np.sum(theta[1:]**2)\n",
    "    return cost + reg'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como em exercícios anteriores, comprovar a sua implementação calculando a função de custo para cada exemplo do dataset.\n",
    "\n",
    "Com o Y correto e a *lambda* a 0, a função de custo também deve ser 0. À medida que o *theta* se afasta ou a *lambda* aumenta, o custo deve ser superior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custo do modelo:\n",
      "0.001630612091949507\n",
      "Theta comprovado e Theta real:\n",
      "[ 9.37670897 12.57506796 16.25229096]\n",
      "[7.33609677 2.25545573 1.12675069]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Comprovar a sua implementação no dataset\n",
    "\n",
    "# Modificar e comprovar vários valores de theta \n",
    "theta = Theta_verd\n",
    "theta = np.array([9.37670897, 12.57506796, 16.25229096])\n",
    "j = regularized_logistic_cost_function(X, Y, Theta_verd, lambda_=0.) \n",
    "\n",
    "print('Custo do modelo:')\n",
    "print(j)\n",
    "print('Theta comprovado e Theta real:') \n",
    "print(theta)\n",
    "print(Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementar a formação por gradient descent\n",
    "\n",
    "Vamos agora otimizar esta função de custos, para formar o nosso modelo através de gradient descent \n",
    "regularizado. \n",
    "\n",
    "No exercício seguinte vamos usar a regularização para efetuar a validação cruzada.\n",
    "\n",
    "Atualizações dos coeficientes *theta*:\n",
    "\n",
    "$\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=0}^{m} (h_\\theta (x^i) - y^i) x_0^i \\\\\n",
    "\\theta_j := \\theta_j - \\alpha [\\frac{1}{m} \\sum\\limits_{i=0}^{m} (h_\\theta (x^i) - y^i) x_0^i + \\frac{\\lambda}{m} \\theta_j]; \\\\\n",
    "j \\in [1, n]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementar a função que forma o modelo por gradient descent regularizado\n",
    "\n",
    "def regularized_logistic_gradient_descent(x, y, theta, alpha=1e-1, lambda_=0., e=0.001, iter_=0.001): \n",
    "    \"\"\" \n",
    "    Formar o modelo otimizando a sua função de custo por gradient descent\n",
    "    \n",
    "    Argumentos posicionais:\n",
    "    x -- array 2D de Numpy com os valores das variáveis independentes dos exemplos, de tamanho m x n \n",
    "    y -- array 1D Numpy com a variável dependente/objetivo, de tamanho m x 1\n",
    "    theta -- array 1D Numpy com os pesos dos coeficientes do modelo, de tamanho 1 x n (vetor fila)\n",
    "    \n",
    "    Argumentos numerados (keyword):\n",
    "    alpha -- float, ratio de formação\n",
    "    lambda -- float com o parâmetro de regularização\n",
    "    e -- float, diferença mínima entre iterações para declarar que a formação finalmente convergiu \n",
    "    iter_ -- int/float, número de iterações\n",
    "    \n",
    "    Devolver:\n",
    "    j_hist -- list/array com a evolução da função de custo durante a formação \n",
    "    theta -- array Numpy com o valor do theta na última iteração\n",
    "    \"\"\"\n",
    "    iter = int(iter_)  # Converter iter_ para inteiro se necessário\n",
    "    j_hist = []  # Inicializar histórico de custos\n",
    "    \n",
    "    m, n = x.shape  # Obter m e n a partir das dimensões de X\n",
    "    \n",
    "    for k in range(iter):  # Iterar sobre o número máximo de iterações\n",
    "        h = sigmoid(theta, x)  # Calcular a hipótese h_theta(x)\n",
    "        error = h - y  # Calcular o erro\n",
    "        \n",
    "        # Atualizar theta\n",
    "        theta_iter = np.copy(theta)  # Copiar theta para atualização\n",
    "        for j in range(n):  # Iterar sobre o número de características\n",
    "            if j == 0:\n",
    "                # Não regularizar o termo de bias\n",
    "                theta_iter[j] = theta[j] - (alpha / m) * np.dot(error, x[:, j])\n",
    "            else:\n",
    "                # Regularizar todos os outros coeficientes\n",
    "                theta_iter[j] = theta[j] - (alpha / m) * (np.dot(error, x[:, j]) + (lambda_ * theta[j]))\n",
    "        \n",
    "        theta = theta_iter  # Atualizar theta para a próxima iteração\n",
    "        \n",
    "        # Calcular o custo para a iteração atual\n",
    "        cost = regularized_logistic_cost_function(x, y, theta, lambda_)\n",
    "        j_hist.append(cost)  # Adicionar o custo ao histórico\n",
    "        \n",
    "        # Verificar a convergência\n",
    "        if k > 0 and abs(j_hist[-1] - j_hist[-2]) < e:\n",
    "            print('Convergir na iteração n.º: ', k)\n",
    "            break\n",
    "    else:\n",
    "        print('N.º máx. de iterações alcançado')\n",
    "    \n",
    "    return j_hist, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formar um modelo de regressão logística não regularizado\n",
    "\n",
    "Para comprovar a implementação da sua função, utilize-o para formar um modelo de regressão logística no dataset sintéticos sem regularização (*lambda* = 0).\n",
    "\n",
    "Comprovar se o modelo converge corretamente para *Theta_verd*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta inicial:\n",
      "[16. 20.  9.]\n",
      "Hiper-parâmetros usados:\n",
      "Alpha: 0.1 Error máx.: 0.01 Nº iter 1000.0\n",
      "Convergir na iteração n.º:  1\n",
      "Tempo de formação (s): 0.003830432891845703\n",
      "\n",
      "Últimos 10 valores da função de custo\n",
      "[0.5983833972268331, 0.5958758607731554]\n",
      "\n",
      "Custo final:\n",
      "0.5958758607731554\n",
      "\n",
      "Theta final:\n",
      "[16.02280998 19.98143377  8.98814716]\n",
      "Valores verdadeiros de Theta e diferença com valores formados:\n",
      "[7.33609677 2.25545573 1.12675069]\n",
      "[ 8.68671321 17.72597804  7.86139647]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Comprovar a sua implementação através da formação de um modelo no dataset sintético anteriormente criado.\n",
    "\n",
    "# Criar um theta inicial com um determinado valor.\n",
    "theta_ini = np.array([16.0, 20.0, 9.0])\n",
    "\n",
    "print('Theta inicial:') \n",
    "print(theta_ini)\n",
    "\n",
    "alpha = 1e-1\n",
    "lambda_ = 0.001 \n",
    "e = 1e-2\n",
    "iter_ = 1e3\n",
    "\n",
    "print('Hiper-parâmetros usados:')\n",
    "print('Alpha:', alpha, 'Error máx.:', e, 'Nº iter', iter_)\n",
    "\n",
    "t = time.time()\n",
    "j_hist, theta_final = regularized_logistic_gradient_descent(X, Y, theta_ini, alpha, lambda_, e, iter_) \n",
    "\n",
    "print('Tempo de formação (s):', time.time() - t)\n",
    "\n",
    "# TODO: completar\n",
    "print('\\nÚltimos 10 valores da função de custo') \n",
    "print(j_hist)\n",
    "print('\\nCusto final:') \n",
    "print(j_hist[-1]) \n",
    "print('\\nTheta final:') \n",
    "print(theta_final)\n",
    "\n",
    "print('Valores verdadeiros de Theta e diferença com valores formados:') \n",
    "print(Theta_verd)\n",
    "print(theta_final - Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representar a evolução da função de custo \n",
    "\n",
    "Para comprovar a evolução da formação do seu modelo, representar graficamente o histórico da função custo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOOD: Representar graficamente a função de custo vs. o número de iterações\n",
    "plt.figure()\n",
    "\n",
    "plt.title('Função de custo') \n",
    "plt.xlabel('nº iterações') \n",
    "plt.ylabel('custo')\n",
    "\n",
    "plt.plot([...]) # Completar\n",
    "\n",
    "plt.grid() \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m54",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m54"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
