{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão logística: Scikit-learn sobre dataset Iris\n",
    "\n",
    "## O que vamos fazer?\n",
    "- Descarregar o dataset Iris.\n",
    "- Pré-processar o dataset usando métodos de Scikit-learn.\n",
    "- Formar um modelo de classificação multiclasse por validação cruzada com Scikit-learn.\n",
    "\n",
    "Agora vamos resolver o mesmo modelo usando métodos de Scikit-learn. \n",
    "\n",
    "Pode ter como referência este exemplo: [Logistic regression 3-class classifier](https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Importar todos os módulos necessários para esta célula\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar o dataset Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Carregar o dataset Iris como arrays X e Y\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processar os dados\n",
    "\n",
    "Pré-processar os dados com métodos de Scikit-learn, do mesmo modo que fez no exercício de Scikit-learn de regressão linear:\n",
    "\n",
    "- Reordená-los aleatoriamente. \n",
    "- Normalizar, se necessário.\n",
    "- Dividi-los em subsets de formação e testes\n",
    "\n",
    "Nesta ocasião, de novo, iremos fazer a validação cruzada por K-fold, já que o dataset é muito pequeno (150 exemplos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.4, 3. , 4.5, 1.5],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [5.1, 3.5, 1.4, 0.2],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [5.9, 3. , 5.1, 1.8],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 2.8, 5.6, 2.1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Reordenar os dados aleatoriamente\n",
    "\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "X = X[indices]\n",
    "Y = Y[indices]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30555556, 0.41666667, 0.59322034, 0.58333333],\n",
       "       [0.        , 0.41666667, 0.01694915, 0.        ],\n",
       "       [0.55555556, 0.58333333, 0.77966102, 0.95833333],\n",
       "       [0.41666667, 0.29166667, 0.69491525, 0.75      ],\n",
       "       [0.66666667, 0.54166667, 0.79661017, 0.83333333],\n",
       "       [1.        , 0.75      , 0.91525424, 0.79166667],\n",
       "       [0.08333333, 0.5       , 0.06779661, 0.04166667],\n",
       "       [0.55555556, 0.54166667, 0.62711864, 0.625     ],\n",
       "       [0.33333333, 0.25      , 0.57627119, 0.45833333],\n",
       "       [0.41666667, 0.29166667, 0.49152542, 0.45833333],\n",
       "       [0.05555556, 0.125     , 0.05084746, 0.08333333],\n",
       "       [0.55555556, 0.20833333, 0.66101695, 0.58333333],\n",
       "       [0.27777778, 0.70833333, 0.08474576, 0.04166667],\n",
       "       [0.66666667, 0.41666667, 0.71186441, 0.91666667],\n",
       "       [0.19444444, 0.        , 0.42372881, 0.375     ],\n",
       "       [0.36111111, 0.29166667, 0.54237288, 0.5       ],\n",
       "       [0.25      , 0.58333333, 0.06779661, 0.04166667],\n",
       "       [0.16666667, 0.41666667, 0.06779661, 0.04166667],\n",
       "       [0.47222222, 0.58333333, 0.59322034, 0.625     ],\n",
       "       [0.58333333, 0.45833333, 0.76271186, 0.70833333],\n",
       "       [0.72222222, 0.45833333, 0.66101695, 0.58333333],\n",
       "       [0.55555556, 0.54166667, 0.84745763, 1.        ],\n",
       "       [0.02777778, 0.5       , 0.05084746, 0.04166667],\n",
       "       [0.22222222, 0.58333333, 0.08474576, 0.04166667],\n",
       "       [0.08333333, 0.58333333, 0.06779661, 0.08333333],\n",
       "       [0.36111111, 0.41666667, 0.59322034, 0.58333333],\n",
       "       [0.30555556, 0.58333333, 0.08474576, 0.125     ],\n",
       "       [0.58333333, 0.29166667, 0.72881356, 0.75      ],\n",
       "       [0.38888889, 1.        , 0.08474576, 0.125     ],\n",
       "       [0.72222222, 0.5       , 0.79661017, 0.91666667],\n",
       "       [0.47222222, 0.41666667, 0.6440678 , 0.70833333],\n",
       "       [0.22222222, 0.625     , 0.06779661, 0.04166667],\n",
       "       [0.47222222, 0.375     , 0.59322034, 0.58333333],\n",
       "       [0.30555556, 0.58333333, 0.11864407, 0.04166667],\n",
       "       [0.36111111, 0.20833333, 0.49152542, 0.41666667],\n",
       "       [0.19444444, 0.125     , 0.38983051, 0.375     ],\n",
       "       [0.25      , 0.29166667, 0.49152542, 0.54166667],\n",
       "       [0.94444444, 0.41666667, 0.86440678, 0.91666667],\n",
       "       [0.83333333, 0.375     , 0.89830508, 0.70833333],\n",
       "       [0.13888889, 0.45833333, 0.10169492, 0.04166667],\n",
       "       [0.38888889, 0.33333333, 0.52542373, 0.5       ],\n",
       "       [0.80555556, 0.5       , 0.84745763, 0.70833333],\n",
       "       [0.66666667, 0.45833333, 0.57627119, 0.54166667],\n",
       "       [0.38888889, 0.33333333, 0.59322034, 0.5       ],\n",
       "       [0.52777778, 0.08333333, 0.59322034, 0.58333333],\n",
       "       [0.22222222, 0.70833333, 0.08474576, 0.125     ],\n",
       "       [0.44444444, 0.41666667, 0.54237288, 0.58333333],\n",
       "       [0.38888889, 0.41666667, 0.54237288, 0.45833333],\n",
       "       [0.30555556, 0.79166667, 0.11864407, 0.125     ],\n",
       "       [0.72222222, 0.45833333, 0.69491525, 0.91666667],\n",
       "       [0.69444444, 0.33333333, 0.6440678 , 0.54166667],\n",
       "       [0.11111111, 0.5       , 0.05084746, 0.04166667],\n",
       "       [0.33333333, 0.16666667, 0.47457627, 0.41666667],\n",
       "       [0.11111111, 0.5       , 0.10169492, 0.04166667],\n",
       "       [0.36111111, 0.41666667, 0.52542373, 0.5       ],\n",
       "       [0.19444444, 0.5       , 0.03389831, 0.04166667],\n",
       "       [0.30555556, 0.79166667, 0.05084746, 0.125     ],\n",
       "       [0.22222222, 0.75      , 0.08474576, 0.08333333],\n",
       "       [0.94444444, 0.25      , 1.        , 0.91666667],\n",
       "       [0.63888889, 0.375     , 0.61016949, 0.5       ],\n",
       "       [0.16666667, 0.20833333, 0.59322034, 0.66666667],\n",
       "       [0.22222222, 0.54166667, 0.11864407, 0.16666667],\n",
       "       [0.86111111, 0.33333333, 0.86440678, 0.75      ],\n",
       "       [0.19444444, 0.58333333, 0.08474576, 0.04166667],\n",
       "       [0.94444444, 0.33333333, 0.96610169, 0.79166667],\n",
       "       [0.08333333, 0.45833333, 0.08474576, 0.04166667],\n",
       "       [0.58333333, 0.375     , 0.55932203, 0.5       ],\n",
       "       [0.47222222, 0.08333333, 0.50847458, 0.375     ],\n",
       "       [0.5       , 0.41666667, 0.61016949, 0.54166667],\n",
       "       [0.38888889, 0.25      , 0.42372881, 0.375     ],\n",
       "       [0.33333333, 0.91666667, 0.06779661, 0.04166667],\n",
       "       [0.66666667, 0.45833333, 0.62711864, 0.58333333],\n",
       "       [0.33333333, 0.125     , 0.50847458, 0.5       ],\n",
       "       [0.44444444, 0.41666667, 0.69491525, 0.70833333],\n",
       "       [0.08333333, 0.66666667, 0.        , 0.04166667],\n",
       "       [0.19444444, 0.41666667, 0.10169492, 0.04166667],\n",
       "       [0.41666667, 0.83333333, 0.03389831, 0.04166667],\n",
       "       [0.69444444, 0.5       , 0.83050847, 0.91666667],\n",
       "       [0.61111111, 0.33333333, 0.61016949, 0.58333333],\n",
       "       [0.22222222, 0.75      , 0.15254237, 0.125     ],\n",
       "       [0.22222222, 0.625     , 0.06779661, 0.08333333],\n",
       "       [0.47222222, 0.29166667, 0.69491525, 0.625     ],\n",
       "       [0.25      , 0.625     , 0.08474576, 0.04166667],\n",
       "       [0.38888889, 0.75      , 0.11864407, 0.08333333],\n",
       "       [0.19444444, 0.58333333, 0.10169492, 0.125     ],\n",
       "       [0.55555556, 0.29166667, 0.66101695, 0.70833333],\n",
       "       [0.33333333, 0.625     , 0.05084746, 0.04166667],\n",
       "       [0.13888889, 0.58333333, 0.10169492, 0.04166667],\n",
       "       [0.41666667, 0.29166667, 0.69491525, 0.75      ],\n",
       "       [0.66666667, 0.41666667, 0.6779661 , 0.66666667],\n",
       "       [0.77777778, 0.41666667, 0.83050847, 0.83333333],\n",
       "       [0.55555556, 0.33333333, 0.69491525, 0.58333333],\n",
       "       [0.55555556, 0.375     , 0.77966102, 0.70833333],\n",
       "       [0.61111111, 0.41666667, 0.81355932, 0.875     ],\n",
       "       [0.5       , 0.375     , 0.62711864, 0.54166667],\n",
       "       [0.58333333, 0.33333333, 0.77966102, 0.875     ],\n",
       "       [0.5       , 0.33333333, 0.62711864, 0.45833333],\n",
       "       [0.5       , 0.41666667, 0.66101695, 0.70833333],\n",
       "       [0.61111111, 0.41666667, 0.76271186, 0.70833333],\n",
       "       [0.58333333, 0.5       , 0.72881356, 0.91666667],\n",
       "       [0.75      , 0.5       , 0.62711864, 0.54166667],\n",
       "       [0.55555556, 0.20833333, 0.6779661 , 0.75      ],\n",
       "       [0.30555556, 0.70833333, 0.08474576, 0.04166667],\n",
       "       [0.02777778, 0.41666667, 0.05084746, 0.04166667],\n",
       "       [0.72222222, 0.45833333, 0.74576271, 0.83333333],\n",
       "       [0.19444444, 0.66666667, 0.06779661, 0.04166667],\n",
       "       [0.36111111, 0.375     , 0.44067797, 0.5       ],\n",
       "       [0.61111111, 0.41666667, 0.71186441, 0.79166667],\n",
       "       [0.41666667, 0.25      , 0.50847458, 0.45833333],\n",
       "       [0.16666667, 0.66666667, 0.06779661, 0.        ],\n",
       "       [0.61111111, 0.5       , 0.69491525, 0.79166667],\n",
       "       [0.52777778, 0.33333333, 0.6440678 , 0.70833333],\n",
       "       [0.13888889, 0.41666667, 0.06779661, 0.08333333],\n",
       "       [0.66666667, 0.45833333, 0.77966102, 0.95833333],\n",
       "       [0.66666667, 0.54166667, 0.79661017, 1.        ],\n",
       "       [0.52777778, 0.58333333, 0.74576271, 0.91666667],\n",
       "       [0.16666667, 0.16666667, 0.38983051, 0.375     ],\n",
       "       [0.13888889, 0.58333333, 0.15254237, 0.04166667],\n",
       "       [0.41666667, 0.29166667, 0.52542373, 0.375     ],\n",
       "       [0.80555556, 0.66666667, 0.86440678, 1.        ],\n",
       "       [0.02777778, 0.375     , 0.06779661, 0.04166667],\n",
       "       [0.33333333, 0.20833333, 0.50847458, 0.5       ],\n",
       "       [0.5       , 0.33333333, 0.50847458, 0.5       ],\n",
       "       [0.19444444, 0.625     , 0.10169492, 0.20833333],\n",
       "       [0.44444444, 0.5       , 0.6440678 , 0.70833333],\n",
       "       [0.91666667, 0.41666667, 0.94915254, 0.83333333],\n",
       "       [0.22222222, 0.20833333, 0.33898305, 0.41666667],\n",
       "       [0.38888889, 0.375     , 0.54237288, 0.5       ],\n",
       "       [0.47222222, 0.08333333, 0.6779661 , 0.58333333],\n",
       "       [0.55555556, 0.125     , 0.57627119, 0.5       ],\n",
       "       [0.16666667, 0.45833333, 0.08474576, 0.        ],\n",
       "       [0.63888889, 0.41666667, 0.57627119, 0.54166667],\n",
       "       [0.5       , 0.25      , 0.77966102, 0.54166667],\n",
       "       [0.94444444, 0.75      , 0.96610169, 0.875     ],\n",
       "       [0.38888889, 0.20833333, 0.6779661 , 0.79166667],\n",
       "       [0.33333333, 0.16666667, 0.45762712, 0.375     ],\n",
       "       [0.22222222, 0.75      , 0.10169492, 0.04166667],\n",
       "       [0.25      , 0.875     , 0.08474576, 0.        ],\n",
       "       [0.19444444, 0.54166667, 0.06779661, 0.04166667],\n",
       "       [0.16666667, 0.45833333, 0.08474576, 0.04166667],\n",
       "       [0.80555556, 0.41666667, 0.81355932, 0.625     ],\n",
       "       [0.58333333, 0.5       , 0.59322034, 0.58333333],\n",
       "       [0.13888889, 0.41666667, 0.06779661, 0.        ],\n",
       "       [0.19444444, 0.625     , 0.05084746, 0.08333333],\n",
       "       [0.66666667, 0.20833333, 0.81355932, 0.70833333],\n",
       "       [0.69444444, 0.41666667, 0.76271186, 0.83333333],\n",
       "       [0.52777778, 0.375     , 0.55932203, 0.5       ],\n",
       "       [0.36111111, 0.33333333, 0.66101695, 0.79166667],\n",
       "       [0.41666667, 0.33333333, 0.69491525, 0.95833333],\n",
       "       [0.58333333, 0.33333333, 0.77966102, 0.83333333]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Normalizar, se necessário\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 4), (30, 4))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Dividi-los em subsets de formação e testes\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formar um modelo inicial\n",
    "- Formar um modelo inicial sobre o subset de formação sem regularização. \n",
    "- Comprovar a idoneidade do modelo e voltar a formá-lo se necessário.\n",
    "\n",
    "A função de Scikit-learn que pode utilizar é [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) com esquema OvR (“one-vs-rest”, uma classe contra o resto).\n",
    "\n",
    "Avaliar sobre o subset de teste com o seu método de *score()*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de treino: 0.9166666666666666\n",
      "Score de teste: 0.7666666666666667\n"
     ]
    }
   ],
   "source": [
    "# TODO: Formar o seu modelo no subconjunto de formação não regularizada e avaliar sobre o de teste\n",
    "\n",
    "clf = LogisticRegression(random_state=42, multi_class='ovr').fit(X_train, y_train)\n",
    "print(\"Score de treino:\", clf.score(X_train, y_train))\n",
    "print(\"Score de teste:\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pode-se ver que o modelo está muito sobreajustado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encontrar a regularização ótima através de validação cruzada\n",
    "- Formar um modelo por cada valor de regularização a considerar.\n",
    "- Forma-os e avalia-os sobre um fold do subset de formação usando K-fold. \n",
    "- Escolher o modelo e a sua regularização ótima.\n",
    "\n",
    "O método LogisticRegression aplica por defeito uma regularização L2 por defeito, mas podemos dar-lhe um valor de *C* determinado. *C* atua como o inverso do fator de regularização *lambda*, pelo que os valores menores, maior regularização:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cs: 10.0\n",
      "Scores: [0.9        0.96666667 0.96666667 0.93333333 0.86666667]\n",
      "Média dos Scores: 0.9266666666666665\n",
      "\n",
      "Cs: 100.0\n",
      "Scores: [1.         1.         0.96666667 0.93333333 0.86666667]\n",
      "Média dos Scores: 0.9533333333333335\n",
      "\n",
      "Cs: 1000.0\n",
      "Scores: [1.         1.         0.96666667 0.93333333 0.86666667]\n",
      "Média dos Scores: 0.9533333333333335\n",
      "\n",
      "Cs: 10000.0\n",
      "Scores: [1.         1.         0.96666667 0.96666667 0.9       ]\n",
      "Média dos Scores: 0.9666666666666668\n",
      "\n",
      "Cs: 99999.99999999999\n",
      "Scores: [1.         1.         0.96666667 0.93333333 0.9       ]\n",
      "Média dos Scores: 0.9600000000000002\n",
      "\n",
      "Acurácias em cada fold: [0.9267, 0.9533, 0.9533, 0.9667, 0.96]\n",
      "Acurácia média: 0.952\n"
     ]
    }
   ],
   "source": [
    "# TODO: Formar um modelo diferente por cada C sobre um fold de K-fold diferente\n",
    "\n",
    "# Usar os valores de lambda que considerávamos em exercícios anteriores\n",
    "lambdas = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "# Calcular o C correspondente a cada um\n",
    "cs = [1/l if l != 0 else 0 for l in lambdas]\n",
    "\n",
    "# Iterar sobre os 5 splits, para os seus modelos e avalia-os no subset do CV gerado\n",
    "log_models = [] \n",
    "best_model = None\n",
    "\n",
    "for c in cs:\n",
    "    model = LogisticRegression(random_state=42, multi_class='ovr', C=c)\n",
    "    scores = cross_val_score(model, X, Y, cv=5, scoring='accuracy')\n",
    "    score = np.mean(scores)\n",
    "    print(\"Cs:\", c)\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Média dos Scores:\", score)\n",
    "    print()\n",
    "    log_models.append(round(score, 4))\n",
    "\n",
    "print(\"Acurácias em cada fold:\", log_models)\n",
    "print(\"Acurácia média:\", np.mean(log_models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliar o modelo finalmente sobre o subset de teste\n",
    "\n",
    "- Mostrar os coeficientes e intercept do melhor modelo. \n",
    "- Avaliar o melhor modelo sobre o subset de teste inicial.\n",
    "- Calcular os acertos e falhas no subset de teste e representá-los graficamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes de intercept do modelo formado\n",
      "Intercept: 4.968756274236784   Coeficientes: [ -5.28870634  11.78220175 -14.90773136 -13.79606419]\n",
      "\n",
      "y_test_pred [0 1 1 2 2 2 0 1 0 2 1 0 0 2 2 1 2 2 0 0 1 2 1 1 2 1 0 1 2 2]\n",
      "y_test [0 1 1 2 2 2 0 1 0 2 1 0 0 2 2 2 2 2 0 0 1 2 1 1 1 1 0 2 2 2]\n",
      "Mean Accuracy: 0.90\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Avaliar o melhor modelo sobre o subset de teste inicial\n",
    "\n",
    "# O melhor modelo foi com lambda = 0.0001\n",
    "model = LogisticRegression(random_state=42, multi_class='ovr', C=cs[3])\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Mostrar os coeficientes e intercept do melhor modelo formado\n",
    "print('Coeficientes de intercept do modelo formado')\n",
    "print(\"Intercept:\", model.intercept_[0], \"  Coeficientes:\", model.coef_[0])  # Mostrar o intercept como o primeiro coeficiente\n",
    "print()  \n",
    "    \n",
    "# Realizar as previsões sobre o subset de teste\n",
    "y_test_pred = model.predict(X_test)\n",
    "print(\"y_test_pred\", y_test_pred)\n",
    "print(\"y_test\", y_test)\n",
    "\n",
    "# Calcular a métrica de avaliação do modelo de precisão (“accuracy”) média (o seu método score())\n",
    "mean_accuracy = model.score(X_test, y_test)\n",
    "print(\"Mean Accuracy: %.2f\" % mean_accuracy)\n",
    "\n",
    "# Calcular os acertos e falhas no subset de teste e representá-los graficamente\n",
    "results = y_test - y_test_pred\n",
    "\n",
    "# Representar graficamente\n",
    "plt.figure(1)\n",
    "\n",
    "plt.scatter(range(len(results)), results)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para concluir comprovou-se que o modelo obteve uma boa acurácia/exatidão de 0.93. Em 30 apenas 3 falharam."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m54",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m54"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
